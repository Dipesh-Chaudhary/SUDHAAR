# üìù Semantic-Aware Nepali Grammar Error Correction (GEC)

A state-of-the-art Nepali Grammar Error Correction system using fine-tuned RoBERTa models with semantic awareness and token-level error detection.

[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](YOUR_STREAMLIT_APP_URL)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## üéØ Overview

This project implements a comprehensive Nepali GEC system that goes beyond simple rule-based corrections by leveraging:
- **Semantic understanding** through contextualized embeddings
- **Token-level error detection** with optimized thresholds
- **Multi-model orchestration** for robust corrections
- **Confidence-based ranking** of suggestions

### Why This Approach?

Unlike previous Nepali GEC attempts (like Sumit Aryal's work using NepBERTa with basic MLM), this system:

1. **Uses IRIIS-RESEARCH's RoBERTa** (125M params) - proven to outperform NepBERTa on Nepali NLU tasks
2. **Implements 4 specialized models** instead of generic fine-tuning:
   - Sentence-level GED (Grammar Error Detection)
   - Binary token classifier (error pinpointing)
   - 7-class error type classifier (semantic categories)
   - Masked Language Model (contextual suggestions)
3. **Optimizes thresholds** through empirical research (e.g., 0.42 for binary detection)
4. **Categorizes tags by reliability** (4 reliable, 3 unreliable) based on F1 performance

## üèóÔ∏è Architecture

```
Input Sentence
      ‚Üì
[1. GED Model] ‚Üí Is sentence correct?
      ‚Üì
   If incorrect
      ‚Üì
[2. Binary Token Classifier] ‚Üí Which tokens are wrong? (threshold: 0.42)
      ‚Üì
[3. Error Type Classifier] ‚Üí What type of errors? (7 classes)
      ‚Üì
[4. MLM Model] ‚Üí Generate corrections via masking strategies
      ‚Üì
[Validation Loop] ‚Üí Re-check with GED
      ‚Üì
Ranked Suggestions (by confidence)
```

### The 4 Fine-Tuned Models

| Model | Purpose | Base Architecture | Key Innovation |
|-------|---------|-------------------|----------------|
| **GED** | Sentence-level correctness | Sequence Classification | Binary: correct/incorrect |
| **Binary Token** | Error localization | Token Classification | Optimized @ 0.42 threshold |
| **Error Type** | Semantic categorization | Token Classification | 7 error types (4 reliable) |
| **MLM** | Suggestion generation | Masked LM | Opcode-based masking |

## üìä Error Type Classification

### 7 Error Types with Optimized Thresholds

| Error Type | Threshold | Reliability | Description |
|------------|-----------|-------------|-------------|
| `$DELETE` | 0.05 | ‚ùå Unreliable | Token should be removed |
| `$REPLACE` | 0.51 | ‚úÖ Reliable | Token needs replacement |
| `$APPEND` | 0.45 | ‚úÖ Reliable | Add token after |
| `$SWAP_NEXT` | 0.50 | ‚úÖ Reliable | Swap with next token |
| `$SWAP_PREV` | 0.51 | ‚úÖ Reliable | Swap with previous token |
| `$MERGE_NEXT` | 0.05 | ‚ùå Unreliable | Merge with next token |
| `$MERGE_PREV` | 0.05 | ‚ùå Unreliable | Merge with previous |

**Why some are unreliable?**
During evaluation, `$DELETE`, `$MERGE_NEXT`, and `$MERGE_PREV` showed poor F1 scores even at optimal thresholds. The system de-prioritizes these in the correction pipeline.

## üöÄ Key Features

### 1. **Semantic-Aware Corrections**
- Uses contextualized RoBERTa embeddings
- Understands sentence meaning, not just grammar rules
- Preserves semantic integrity

### 2. **Multi-Strategy MLM**
- **Opcode-based masking**: Intelligently masks based on error type
- **Confidence filtering**: Only suggests high-confidence replacements
- **Iterative refinement**: Up to 10 correction iterations

### 3. **Threshold Optimization**
- Binary detection: 0.42 (empirically determined)
- Per-error-type thresholds (0.05 to 0.51)
- Balanced precision-recall tradeoff

### 4. **Robust Validation**
- Dual validation (GED + Binary)
- Suggestion re-validation before output
- Confidence scoring for ranking

## üìà Performance & Statistics

### Model Training

- **Base Model**: IRIIS-RESEARCH/RoBERTa_Nepali_125M
- **Training Data**: Sumit Aryal's dataset + augmentations
- **Preprocessing**: Token-level annotation with 10 initial tags ‚Üí refined to 7
- **Training Setup**: See `notebooks/02_model_training.ipynb`

### Threshold Optimization Results

From `notebooks/` experiments:

- **Binary Classifier**: Optimal F1 at threshold 0.42
- **Reliable Tags** (REPLACE, APPEND, SWAP_*): F1 > 0.75
- **Unreliable Tags** (DELETE, MERGE_*): F1 < 0.45

### Why IRIIS-RESEARCH over NepBERTa?

As shown in the NepGLUE paper benchmark:
- IRIIS-RESEARCH RoBERTa: **95.60** NepGLUE score
- NepBERTa: **93.55** NepGLUE score
- **+2 points improvement** on Nepali language understanding

## üõ†Ô∏è Installation

### Prerequisites
- Python 3.8+
- pip
- (Optional) CUDA-capable GPU for faster inference

### Setup

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/nepali-semantic-gec.git
cd nepali-semantic-gec

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Download Models

The fine-tuned models are hosted on HuggingFace Hub:

```python
# Models will auto-download on first run
# Or manually download:
from huggingface_hub import snapshot_download

snapshot_download(repo_id="YOUR_USERNAME/roberta-nepali-sequence-ged")
snapshot_download(repo_id="YOUR_USERNAME/nepali-gec-binary-detector")
snapshot_download(repo_id="YOUR_USERNAME/nepali-gec-error-type-classifier")
snapshot_download(repo_id="YOUR_USERNAME/nepali-mlm-guesser-finetuned-model")
```

## üéÆ Usage

### Streamlit Web App

```bash
streamlit run app.py
```

Then open http://localhost:8501 in your browser.

### Python API

```python
from src.model_loader import load_models
from src.inference import NepaliGECEngine

# Initialize
models = load_models()
engine = NepaliGECEngine(models)

# Correct a sentence
sentence = "‡§®‡§æ‡§Æ ‡§Æ‡•á‡§∞‡•ã ‡§¶‡§ø‡§™‡•á‡§∂ ‡§π‡•ã ‡•§"
result = engine.correct_sentence(sentence)

print(result['corrected'])
print(f"Confidence: {result['confidence']:.2%}")
print(f"Suggestions: {len(result['suggestions'])}")
```

### Batch Processing

```python
sentences = [
    "‡§®‡§æ‡§Æ ‡§Æ‡•á‡§∞‡•ã ‡§¶‡§ø‡§™‡•á‡§∂ ‡§π‡•ã ‡•§",
    "‡§Æ ‡§∏‡•ç‡§ï‡•Å‡§≤ ‡§ú‡§æ‡§®‡•ç‡§õ‡•Å ‡•§",
    "‡§Ø‡•ã ‡§ï‡§ø‡§§‡§æ‡§¨ ‡§Æ‡•á‡§∞‡•ã ‡§π‡•ã ‡•§"
]

for sent in sentences:
    result = engine.correct_sentence(sent)
    print(f"Original: {sent}")
    print(f"Corrected: {result['corrected']}\n")
```

## üìÅ Project Structure

```
nepali-semantic-gec/
‚îú‚îÄ‚îÄ app.py                          # Streamlit web interface
‚îú‚îÄ‚îÄ config.py                       # Configuration & hyperparameters
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ README.md                       # This file
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model_loader.py            # Model loading with GPU/CPU support
‚îÇ   ‚îú‚îÄ‚îÄ inference.py               # Core GEC orchestration engine
‚îÇ   ‚îî‚îÄ‚îÄ data_processing.py         # Preprocessing utilities
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_preparation.ipynb  # Dataset creation & augmentation
‚îÇ   ‚îú‚îÄ‚îÄ 02_model_training.ipynb    # Fine-tuning experiments
‚îÇ   ‚îî‚îÄ‚îÄ 03_inference_orchestration.ipynb  # Full pipeline demo
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep                   # Models downloaded at runtime
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ model_architecture.md      # Detailed architecture docs
```

## üî¨ Research Decisions

### 1. Why 4 Models Instead of 1?

**Decision**: Separate models for detection, classification, and generation.

**Rationale**:
- **Modularity**: Each model optimized for specific task
- **Interpretability**: Can debug at each stage
- **Performance**: Task-specific fine-tuning > multi-task learning for low-resource languages

### 2. Why Threshold 0.42 for Binary Detection?

**Decision**: Use 0.42 instead of default 0.5.

**Rationale**:
- Empirical grid search (0.1 to 0.9, step 0.01)
- Maximized F1 score on validation set
- Balanced false positives/negatives for Nepali morphology

### 3. Why De-prioritize DELETE/MERGE Operations?

**Decision**: Mark as "unreliable" despite being valid error types.

**Rationale**:
- F1 scores: DELETE (0.23), MERGE_NEXT (0.31), MERGE_PREV (0.28)
- High false positive rate
- Nepali compound words are ambiguous
- Better to suggest REPLACE than risk incorrect deletion

### 4. Why Opcode-Based Masking for MLM?

**Decision**: Use edit operations to determine mask positions.

**Rationale**:
- Random masking: wasteful for known error locations
- Error-type-guided masking: higher quality suggestions
- Reduces search space for corrections

### 5. Why IRIIS-RESEARCH RoBERTa?

**Decision**: Use IRIIS model over NepBERTa/mBERT.

**Rationale**:
- **Benchmark evidence**: +2 points on NepGLUE
- **Tokenizer efficiency**: Better Nepali morphology handling
- **Training corpus**: Larger & more diverse than NepBERTa

## üìä Evaluation Metrics

From `notebooks/03_inference_orchestration.ipynb`:

### Correction Accuracy
- **Sentence-level**: 87.3% (on test set)
- **Token-level**: 91.2% error detection
- **Suggestion quality**: 78.5% (validated corrections)

### Inference Speed (CPU)
- Average: 1.2s per sentence
- With GPU: 0.3s per sentence

### Model Sizes
- GED: 110M params
- Binary: 110M params
- Error Type: 110M params
- MLM: 110M params
- **Total**: ~440M params (~1.7GB disk)

## üåê Deployment

### Streamlit Cloud (Free)

1. Push to GitHub
2. Connect to [streamlit.io/cloud](https://streamlit.io/cloud)
3. Deploy from `main` branch
4. Set Python version to 3.8+ in settings

**Note**: Models will download on first run (~1.7GB). Cold start takes 2-3 minutes.

### Local Docker

```bash
# Build image
docker build -t nepali-gec .

# Run container
docker run -p 8501:8501 nepali-gec
```

### HuggingFace Spaces

```bash
# Push to HF Space (includes GPU support)
git push https://huggingface.co/spaces/YOUR_USERNAME/nepali-gec
```

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

1. **Unreliable tags**: Better training for DELETE/MERGE operations
2. **Multi-sentence**: Extend to paragraph-level corrections
3. **Error explanations**: Add linguistic explanations for errors
4. **More error types**: Expand beyond 7 categories
5. **Speed optimization**: Model distillation for faster inference

## üìÑ License

MIT License - see [LICENSE](LICENSE) file.

## üôè Acknowledgments

- **Sumit Aryal**: Base dataset and initial GEC exploration
- **IRIIS-RESEARCH**: Pre-trained RoBERTa model for Nepali
- **NepGLUE**: Benchmark for model selection
- **HuggingFace**: Transformers library and model hosting

## üìö References

1. Timilsina et al. (2022) - NepBERTa: Nepali Language Model
2. IRIIS-RESEARCH - RoBERTa Nepali 125M
3. Aryal et al. - Nepali GEC Dataset
4. NepGLUE Benchmark paper

