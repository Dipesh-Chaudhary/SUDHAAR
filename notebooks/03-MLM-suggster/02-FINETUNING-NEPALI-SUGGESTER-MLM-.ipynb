{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.57.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: packaging in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 0.26.0 not found\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from typing import Dict, Any, Union, List\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DefaultDataCollator, #\n",
    "    BatchEncoding\n",
    ")\n",
    "from huggingface_hub import login, HfFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "HF_USERNAME = \"DipeshChaudhary\"\n",
    "MODEL_NAME = \"IRIIS-RESEARCH/RoBERTa_Nepali_125M\"\n",
    "DATASET_NAME = f\"{HF_USERNAME}/nepali-gector-mlm-guesser-dataset\" \n",
    "OUTPUT_DIR = \"nepali-mlm-guesser-finetuned-model-1\"\n",
    "HUB_MODEL_ID = f\"{HF_USERNAME}/{OUTPUT_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP16_MODE = 'bf16' \n",
    "PER_DEVICE_BATCH_SIZE = 512 \n",
    "GRADIENT_ACCUMULATION = 1\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 3\n",
    "MAX_STEPS = -1 # Run for all epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hugging Face login successful.\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"âœ… Hugging Face login successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LOADING MODEL: IRIIS-RESEARCH/RoBERTa_Nepali_125M\n",
      "==================================================\n",
      "LOADING DATASET: DipeshChaudhary/nepali-gector-mlm-guesser-dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"LOADING MODEL: {MODEL_NAME}\")\n",
    "print(\"=\"*50)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load the prepared MLM dataset (containing 'input_ids' and 'labels')\n",
    "print(f\"LOADING DATASET: {DATASET_NAME}\")\n",
    "try:\n",
    "    tokenized_datasets = load_dataset(DATASET_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR loading dataset. Ensure {DATASET_NAME} is uploaded to the Hub.\")\n",
    "    print(e)\n",
    "    tokenized_datasets = None\n",
    "\n",
    "if not tokenized_datasets:\n",
    "    raise FileNotFoundError(f\"Could not load dataset {DATASET_NAME}. Please check the repo ID.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed non-essential columns: ['incorrect_sentence', 'correct_sentence', 'mlm_tag_count']\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"mlm_input_ids\", \"input_ids\")\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"mlm_labels\", \"labels\")\n",
    "\n",
    "columns_to_remove = [col for col in tokenized_datasets['train'].column_names if col not in ['input_ids', 'labels', 'attention_mask']]\n",
    "if columns_to_remove:\n",
    "    # Need to handle potential KeyError if attention_mask is already gone\n",
    "    try:\n",
    "        tokenized_datasets = tokenized_datasets.remove_columns(columns_to_remove)\n",
    "        print(f\"Removed non-essential columns: {columns_to_remove}\")\n",
    "    except KeyError:\n",
    "        # If the column wasn't there to begin with, ignore the error\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating missing 'attention_mask' column...\n"
     ]
    }
   ],
   "source": [
    "def create_attention_mask(examples):\n",
    "    \"\"\"\n",
    "    Generates a full attention mask (all 1s) for non-padded tokens.\n",
    "    This is necessary because the input dataset is missing this column.\n",
    "    \"\"\"\n",
    "    # Create an attention mask of 1s for the length of the existing input_ids\n",
    "    examples['attention_mask'] = [1] * len(examples['input_ids'])\n",
    "    return examples\n",
    "\n",
    "# Check if attention_mask is missing before generating it\n",
    "if 'attention_mask' not in tokenized_datasets['train'].column_names:\n",
    "    print(\"\\nGenerating missing 'attention_mask' column...\")\n",
    "    # Apply the function across all splits\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        create_attention_mask,\n",
    "        num_proc=os.cpu_count(),\n",
    "        desc=\"Generating attention_mask\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n'attention_mask' column already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying fixed-length padding (MAX_SEQ_LENGTH=128) to dataset...\n"
     ]
    }
   ],
   "source": [
    "def pad_data(examples):\n",
    "    \"\"\"\n",
    "    Pads the input_ids, attention_mask, and labels to the fixed MAX_SEQ_LENGTH (128)\n",
    "    and sets the label padding tokens to -100.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate length of the current sequence\n",
    "    current_length = len(examples['input_ids'])\n",
    "    \n",
    "    # Calculate how much padding is needed\n",
    "    padding_needed = MAX_SEQ_LENGTH - current_length\n",
    "\n",
    "    if padding_needed > 0:\n",
    "        # 1. Pad input_ids (using the tokenizer's padding ID)\n",
    "        examples['input_ids'] = examples['input_ids'] + [tokenizer.pad_token_id] * padding_needed\n",
    "        \n",
    "        # 2. Pad attention_mask (using 0s, since padded tokens should not be attended to)\n",
    "        examples['attention_mask'] = examples['attention_mask'] + [0] * padding_needed\n",
    "        \n",
    "        # 3. Pad labels (using -100, so the loss function ignores them)\n",
    "        examples['labels'] = examples['labels'] + [-100] * padding_needed\n",
    "    \n",
    "    # IMPORTANT: Truncate any sequences longer than MAX_SEQ_LENGTH\n",
    "    # This is a safety measure.\n",
    "    if current_length > MAX_SEQ_LENGTH:\n",
    "        examples['input_ids'] = examples['input_ids'][:MAX_SEQ_LENGTH]\n",
    "        examples['attention_mask'] = examples['attention_mask'][:MAX_SEQ_LENGTH]\n",
    "        examples['labels'] = examples['labels'][:MAX_SEQ_LENGTH]\n",
    "\n",
    "    return examples\n",
    "\n",
    "print(f\"\\nApplying fixed-length padding (MAX_SEQ_LENGTH={MAX_SEQ_LENGTH}) to dataset...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CRITICAL FIX: Switched to DefaultDataCollator to preserve pre-set labels.\n",
      "\n",
      "Dataset features (MUST contain 'input_ids', 'labels', and 'attention_mask'):\n",
      "['input_ids', 'labels', 'attention_mask']\n",
      "\n",
      "==================================================\n",
      "TRAINING ARGUMENTS (H100 OPTIMIZED)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.map(\n",
    "    pad_data,\n",
    "    num_proc=os.cpu_count(), # Use all available cores for fast processing\n",
    "    desc=f\"Padding to fixed length {MAX_SEQ_LENGTH}\"\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"pt\") \n",
    "\n",
    "print(f\"\\nCRITICAL FIX: Switched to DefaultDataCollator to preserve pre-set labels.\")\n",
    "\n",
    "# Let's verify the feature names in the dataset:\n",
    "print(\"\\nDataset features (MUST contain 'input_ids', 'labels', and 'attention_mask'):\")\n",
    "print(tokenized_datasets['train'].column_names)\n",
    "\n",
    "# --- 6. TRAINING ARGUMENTS ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING ARGUMENTS (H100 OPTIMIZED)\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    max_steps=MAX_STEPS, \n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE * 2, \n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    \n",
    "    bf16=(FP16_MODE == 'bf16'), \n",
    "    # fp16=(FP16_MODE == 'fp16')\n",
    "    \n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Checkpointing and Logging\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=False, # We usually want the last checkpoint for finetuning\n",
    "    save_total_limit=1, # Save only the last checkpoint\n",
    "    \n",
    "    # Uploading to Hub\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_MODEL_ID,\n",
    "    hub_token=HF_TOKEN,\n",
    "    hub_private_repo=True,\n",
    "    \n",
    "    # CPU usage - utilize all available cores\n",
    "    dataloader_num_workers=os.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "STARTING MLM FINE-TUNING\n",
      "Effective Batch Size: 512\n",
      "Precision: BF16\n",
      "Forced Max Sequence Length: 128\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24036' max='24036' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24036/24036 1:29:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.754820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>0.482485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.428481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ‰ TRAINING COMPLETE!\n",
      "\n",
      "ðŸŽ‰ TRAINING COMPLETE!\n",
      "\n",
      "==================================================\n",
      "FINE-TUNING SCRIPT EXECUTION FINISHED.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STARTING MLM FINE-TUNING\")\n",
    "print(f\"Effective Batch Size: {PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"Precision: {FP16_MODE.upper()}\")\n",
    "print(f\"Forced Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "print(\"=\"*50)\n",
    "train_result = trainer.train()\n",
    "print(\"\\nðŸŽ‰ TRAINING COMPLETE!\")\n",
    "print(\"\\nðŸŽ‰ TRAINING COMPLETE!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINE-TUNING SCRIPT EXECUTION FINISHED.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SAVING AND UPLOADING FINAL MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e04b079129249a58036af81aecbc967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd98e1b24d2455a81be7a0b86aa5614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda1590d8bf940fda9a01e022e7d527e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecad71ad18e4bd9b8d000a56c9ae0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully pushed to the Hub at: DipeshChaudhary/nepali-mlm-guesser-finetuned-model-1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING AND UPLOADING FINAL MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save the final model locally\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "trainer.push_to_hub(commit_message=\"MLM Guesser fine-tuning completed \")\n",
    "print(f\"Model successfully pushed to the Hub at: {HUB_MODEL_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
