{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NEPALI GEC DATASET PROCESSOR\n",
        "#\n",
        "# This script converts a raw Nepali GEC dataset into a token-level\n",
        "# (subword) dataset with 10 robust GEC tags.\n",
        "#\n",
        "# This uses a strict multi-pass priority system to\n",
        "# correctly tag all operations, including complex non-difflib-aligned\n",
        "# edits like SWAP, MERGE, and SPLIT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "%uv pip install transformers datasets accelerate evaluate scikit-learn wandb huggingface_hub"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 37ms\u001b[0m\u001b[0m\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import gc\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import difflib\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Tuple\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import datasets\n",
        "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import HfApi, create_repo, login"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# HF_TOKEN = \"\" "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "MODEL_NAME = \"IRIIS-RESEARCH/RoBERTa_Nepali_125M\"\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "HF_USERNAME = \"DipeshChaudhary\"\n",
        "RAW_DATASET_NAME = \"sumitaryal/nepali_grammatical_error_correction\"\n",
        "FINAL_DATASET_NAME = \"nepali-gector-style-token-level-tag-for-ged\" \n",
        "REPO_ID = f\"{HF_USERNAME}/{FINAL_DATASET_NAME}\"\n",
        "LOCAL_DATA_PATH = \"./nepali-gector-style-token-level-tag-for-ged\"\n",
        "VOCAB_FILENAME = \"gec_vocabulary.json\"\n",
        "NUM_WORKERS = max(1, cpu_count() - 2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(f\"--- NEPALI GEC DATASET PROCESSOR ---\")\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Max Seq Length: {MAX_SEQUENCE_LENGTH}\")\n",
        "print(f\"Workers: {NUM_WORKERS}\")\n",
        "print(f\"Output Repo: {REPO_ID}\")\n",
        "print(f\"Local Path: {LOCAL_DATA_PATH}\")\n",
        "print(\"-\" * 70 + \"\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEPALI GEC DATASET PROCESSOR ---\n",
            "Model: IRIIS-RESEARCH/RoBERTa_Nepali_125M\n",
            "Max Seq Length: 128\n",
            "Workers: 106\n",
            "Output Repo: DipeshChaudhary/nepali-gector-style-token-level-tag-for-ged\n",
            "Local Path: ./nepali-gector-style-token-level-tag-for-ged\n",
            "----------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "class EnhancedNepaliGECVocabulary:\n",
        "    \"\"\"Defines the mapping between GEC tag names and their integer IDs.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.KEEP_ID = 0\n",
        "        self.DELETE_ID = 1\n",
        "        self.REPLACE_ID = 2\n",
        "        self.APPEND_ID = 3\n",
        "        self.SWAP_NEXT_ID = 4\n",
        "        self.SWAP_PREV_ID = 5\n",
        "        self.MERGE_NEXT_ID = 6\n",
        "        self.MERGE_PREV_ID = 7\n",
        "        self.SPLIT_ID = 8\n",
        "        self.UNKNOWN_ID = 9\n",
        "        \n",
        "        self.tag_to_id = {\n",
        "            \"$KEEP\": self.KEEP_ID, \"$DELETE\": self.DELETE_ID,\n",
        "            \"$REPLACE\": self.REPLACE_ID, \"$APPEND\": self.APPEND_ID,\n",
        "            \"$SWAP_NEXT\": self.SWAP_NEXT_ID, \"$SWAP_PREV\": self.SWAP_PREV_ID,\n",
        "            \"$MERGE_NEXT\": self.MERGE_NEXT_ID, \"$MERGE_PREV\": self.MERGE_PREV_ID,\n",
        "            \"$SPLIT\": self.SPLIT_ID, \"$UNKNOWN\": self.UNKNOWN_ID\n",
        "        }\n",
        "        self.id_to_tag = {v: k for k, v in self.tag_to_id.items()}\n",
        "    \n",
        "    def get_tag_name(self, tag_id: int) -> str:\n",
        "        return self.id_to_tag.get(tag_id, \"$UNKNOWN\")\n",
        "    \n",
        "    def get_id(self, tag_name: str) -> int:\n",
        "        return self.tag_to_id.get(tag_name, self.UNKNOWN_ID)\n",
        "    \n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.tag_to_id)\n",
        "    \n",
        "    def save(self, filepath: str):\n",
        "        data = {\n",
        "            \"tag_to_id\": self.tag_to_id,\n",
        "            \"id_to_tag\": {int(k): v for k, v in self.id_to_tag.items()}\n",
        "        }\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"\u2713 Vocabulary saved: {filepath}\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# --- ALIGNMENT & TAGGING FUNCTIONS ---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def calculate_levenshtein_opcodes(incorrect_words: List[str], correct_words: List[str]) -> List[Tuple[str, int, int, int, int]]:\n",
        "    \"\"\"Calculates Levenshtein opcodes (tag, i1, i2, j1, j2) using difflib.\"\"\"\n",
        "    s = difflib.SequenceMatcher(None, incorrect_words, correct_words, autojunk=False)\n",
        "    return s.get_opcodes()\n",
        "\n",
        "def generate_word_level_tags(incorrect_words: List[str], \n",
        "                             correct_words: List[str],\n",
        "                             vocabulary: EnhancedNepaliGECVocabulary) -> Tuple[List[int], Dict]:\n",
        "    \"\"\"\n",
        "    Generates robust, word-level GEC tags using a strict multi-pass \n",
        "    priority system (V15 logic).\n",
        "    \n",
        "    Priority Order:\n",
        "    1. SWAP (Content-based)\n",
        "    2. MERGE (Content-based)\n",
        "    3. Opcode-based (APPEND, SPLIT, DELETE, REPLACE, KEEP)\n",
        "    \"\"\"\n",
        "    opcodes = calculate_levenshtein_opcodes(incorrect_words, correct_words)\n",
        "    tags = [vocabulary.KEEP_ID] * len(incorrect_words)\n",
        "    \n",
        "    # --- PASS 1: GLOBAL SWAP Detection (Highest Priority) ---\n",
        "    # This pass ignores opcodes and tags based on content match.\n",
        "    i1 = 0\n",
        "    while i1 < len(incorrect_words):\n",
        "        if tags[i1] != vocabulary.KEEP_ID: i1 += 1; continue\n",
        "        i2 = i1 + 1\n",
        "        while i2 < len(incorrect_words):\n",
        "            if tags[i2] != vocabulary.KEEP_ID: i2 += 1; continue\n",
        "            \n",
        "            j1, j2 = -1, -1\n",
        "            try:\n",
        "                # Find inc[i1] in correct sentence\n",
        "                j2 = correct_words.index(incorrect_words[i1]) \n",
        "                # Find inc[i2] in correct sentence\n",
        "                j1 = correct_words.index(incorrect_words[i2]) \n",
        "            except ValueError:\n",
        "                pass # One or both words not in correct sentence\n",
        "\n",
        "            # CRITICAL SWAP CONDITION:\n",
        "            # inc[i1] is at cor[j2]\n",
        "            # inc[i2] is at cor[j1]\n",
        "            # ... and their order is reversed (j1 < j2)\n",
        "            if j1 != -1 and j2 != -1 and j1 < j2:\n",
        "                tags[i1] = vocabulary.SWAP_NEXT_ID\n",
        "                tags[i2] = vocabulary.SWAP_PREV_ID\n",
        "                break # Found swap for i1, break inner loop\n",
        "            i2 += 1\n",
        "        i1 += 1\n",
        "\n",
        "    # --- PASS 2: MERGE Detection (Second Highest Priority) ---\n",
        "    # This pass also ignores opcodes and tags based on content.\n",
        "    i = 0\n",
        "    while i < len(incorrect_words) - 1:\n",
        "        # Check if current and next word are untagged\n",
        "        if tags[i] == vocabulary.KEEP_ID and tags[i+1] == vocabulary.KEEP_ID:\n",
        "            merged_inc_word = incorrect_words[i] + incorrect_words[i+1]\n",
        "            # Check if this merged word exists in the correct sentence\n",
        "            if merged_inc_word in correct_words:\n",
        "                tags[i] = vocabulary.MERGE_NEXT_ID\n",
        "                tags[i+1] = vocabulary.MERGE_PREV_ID\n",
        "                i += 2 # Skip both words\n",
        "                continue\n",
        "        i += 1\n",
        "\n",
        "    # --- PASS 3: Opcode Loop (SPLIT, DELETE, REPLACE, APPEND, KEEP) ---\n",
        "    # This pass applies tags ONLY if a higher-priority tag (SWAP, MERGE)\n",
        "    # has not already been set.\n",
        "    \n",
        "    # We must process opcodes in a specific order:\n",
        "    # 1. Insertions (for APPEND)\n",
        "    # 2. Equal/Replace/Delete (for all other tags)\n",
        "    \n",
        "    # Process Insertions first to fix the Append-Overwrite bug\n",
        "    for op, i_start, i_end, c_start, c_end in opcodes:\n",
        "        if op == 'insert':\n",
        "            target_idx = max(0, i_start - 1)\n",
        "            # Only tag if the target is valid and is currently KEEP\n",
        "            if 0 <= target_idx < len(tags) and tags[target_idx] == vocabulary.KEEP_ID: \n",
        "                 tags[target_idx] = vocabulary.APPEND_ID\n",
        "\n",
        "    # Process all other operations\n",
        "    for op, i_start, i_end, c_start, c_end in opcodes:\n",
        "        if op == 'insert':\n",
        "            continue # Already handled\n",
        "\n",
        "        # Iterate over the affected index range in incorrect_words\n",
        "        for idx in range(i_start, i_end):\n",
        "            # CRITICAL BUG FIX:\n",
        "            # Do NOT overwrite a tag from a higher-priority pass\n",
        "            if tags[idx] != vocabulary.KEEP_ID:\n",
        "                continue\n",
        "\n",
        "            # Now, apply the tag based on the opcode\n",
        "            if op == 'equal':\n",
        "                tags[idx] = vocabulary.KEEP_ID\n",
        "            \n",
        "            elif op == 'delete':\n",
        "                tags[idx] = vocabulary.DELETE_ID\n",
        "            \n",
        "            elif op == 'replace':\n",
        "                inc_len, cor_len = i_end - i_start, c_end - c_start\n",
        "                # Check for SPLIT\n",
        "                if inc_len == 1 and cor_len > 1:\n",
        "                    inc_word = incorrect_words[i_start]\n",
        "                    cor_words_merged = \"\".join(correct_words[c_start:c_end])\n",
        "                    if inc_word == cor_words_merged:\n",
        "                        tags[idx] = vocabulary.SPLIT_ID\n",
        "                        continue # Skip the general replace below\n",
        "                \n",
        "                # General Replace\n",
        "                tags[idx] = vocabulary.REPLACE_ID\n",
        "\n",
        "    # --- PASS 4: Final Stat Recalculation ---\n",
        "    final_stats = Counter([vocabulary.get_tag_name(t).replace('$', '').lower() for t in tags if t != vocabulary.KEEP_ID])\n",
        "    final_stats['keep'] = tags.count(vocabulary.KEEP_ID)\n",
        "\n",
        "    return tags, dict(final_stats)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "tokenizer = None\n",
        "vocabulary = None\n",
        "\n",
        "def preprocess_function(examples: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Processes a batch of examples, generating WORD-level tags\n",
        "    and mapping them to SUBWORD (token-level) labels for the model.\n",
        "    \"\"\"\n",
        "    incorrect_texts = [str(s).strip() for s in examples[\"incorrect_sentence\"]]\n",
        "    correct_texts = [str(s).strip() for s in examples[\"correct_sentence\"]]\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_labels = []\n",
        "    all_is_correct = []\n",
        "    all_tag_stats = []\n",
        "    all_stratify_keys = []\n",
        "\n",
        "    for inc_text, cor_text in zip(incorrect_texts, correct_texts):\n",
        "        \n",
        "        dominant_tag = vocabulary.KEEP_ID # Default stratification key\n",
        "        \n",
        "        # --- 1. Process INCORRECT sentence ---\n",
        "        if inc_text and inc_text != cor_text and len(inc_text) > 1 and len(cor_text) > 1:\n",
        "            \n",
        "            inc_words = inc_text.split()\n",
        "            cor_words = cor_text.split()\n",
        "            \n",
        "            # Skip if splitting results in empty lists\n",
        "            if not inc_words or not cor_words:\n",
        "                continue\n",
        "                \n",
        "            word_tags, stats = generate_word_level_tags(inc_words, cor_words, vocabulary)\n",
        "            \n",
        "            encoding = tokenizer(\n",
        "                inc_text,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=MAX_SEQUENCE_LENGTH,\n",
        "                return_special_tokens_mask=True\n",
        "            )\n",
        "            \n",
        "            labels = []\n",
        "            word_ids = encoding.word_ids()\n",
        "            \n",
        "            previous_word_idx = None\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    labels.append(-100)\n",
        "                elif word_idx < len(word_tags):\n",
        "                    if word_idx != previous_word_idx:\n",
        "                        labels.append(word_tags[word_idx])\n",
        "                    else:\n",
        "                        labels.append(vocabulary.KEEP_ID)\n",
        "                else:\n",
        "                    labels.append(-100)\n",
        "                \n",
        "                previous_word_idx = word_idx\n",
        "\n",
        "            if stats:\n",
        "                error_tags = {k: v for k, v in stats.items() if k != 'keep'}\n",
        "                if error_tags:\n",
        "                    dominant_tag_name = max(error_tags, key=error_tags.get)\n",
        "                    dominant_tag = vocabulary.get_id(f\"${dominant_tag_name.upper()}\")\n",
        "            \n",
        "            all_input_ids.append(encoding['input_ids'])\n",
        "            all_attention_masks.append(encoding['attention_mask'])\n",
        "            all_labels.append(labels)\n",
        "            all_is_correct.append(False)\n",
        "            all_tag_stats.append(json.dumps(stats))\n",
        "            all_stratify_keys.append(str(dominant_tag))\n",
        "\n",
        "        # --- 2. Process CORRECT sentence ---\n",
        "        if cor_text and len(cor_text) > 1:\n",
        "            encoding = tokenizer(\n",
        "                cor_text,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=MAX_SEQUENCE_LENGTH\n",
        "            )\n",
        "            labels = []\n",
        "            keep_count = 0\n",
        "            word_ids = encoding.word_ids()\n",
        "            for word_id in word_ids:\n",
        "                if word_id is None:\n",
        "                    labels.append(-100)\n",
        "                else:\n",
        "                    labels.append(vocabulary.KEEP_ID)\n",
        "                    keep_count += 1\n",
        "            \n",
        "            all_input_ids.append(encoding['input_ids'])\n",
        "            all_attention_masks.append(encoding['attention_mask'])\n",
        "            all_labels.append(labels)\n",
        "            all_is_correct.append(True)\n",
        "            all_tag_stats.append(json.dumps({\"keep\": keep_count}))\n",
        "            all_stratify_keys.append(str(vocabulary.KEEP_ID)) # Stratify key for correct\n",
        "            \n",
        "    # Return batch\n",
        "    return {\n",
        "        \"input_ids\": all_input_ids,\n",
        "        \"attention_mask\": all_attention_masks,\n",
        "        \"labels\": all_labels,\n",
        "        \"is_correct\": all_is_correct,\n",
        "        \"tag_stats\": all_tag_stats,\n",
        "        \"stratify_key\": all_stratify_keys\n",
        "    }"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def run_pipeline():\n",
        "    \"\"\"Executes the full dataset processing pipeline.\"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STEP 1: INITIALIZING TOKENIZER & VOCABULARY\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    global tokenizer, vocabulary\n",
        "    try:\n",
        "        # Set TOKENIZERS_PARALLELISM to false to avoid warnings\n",
        "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        vocabulary = EnhancedNepaliGECVocabulary()\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Could not load tokenizer '{MODEL_NAME}'. {e}\")\n",
        "        print(\"Please ensure 'transformers' is installed and you have internet access.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\u2713 Tokenizer loaded: {MODEL_NAME}\")\n",
        "    print(f\"\u2713 Vocabulary loaded: {vocabulary.vocab_size()} tags\")\n",
        "    print(f\"Tags: {list(vocabulary.tag_to_id.keys())}\\n\")\n",
        "\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STEP 2: VERIFYING TAGGING LOGIC (COMPREHENSIVE TEST)\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Comprehensive Test Cases (Nepali)\n",
        "    test_cases_comprehensive = [\n",
        "        (\"Merge\", \"\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0901\u0926\u0948 \u091b\u0941\", \"\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0901\u0926\u0948\u091b\u0941\"), \n",
        "        (\"Split\", \"\u0924\u092a\u093e\u0908\u0902\u0915\u094b \u0928\u093e\u092e\u0915\u0947 \u0939\u094b\", \"\u0924\u092a\u093e\u0908\u0902\u0915\u094b \u0928\u093e\u092e \u0915\u0947 \u0939\u094b\"),\n",
        "        (\"Swap_Adj\", \"\u0916\u093e\u0928\u093e \u092e \u0916\u093e\u0928\u094d\u091b\u0941\", \"\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0928\u094d\u091b\u0941\"), \n",
        "        (\"Swap_Complex\", \"\u0938\u094b\u0927\u094d\u0928 \u0915\u0947\u0939\u0940 \u091b \u092e\u0932\u093e\u0908\", \"\u092e\u0932\u093e\u0908 \u0938\u094b\u0927\u094d\u0928 \u0915\u0947\u0939\u0940 \u091b\"),\n",
        "        (\"Append (End)\", \"\u092e \u0915\u0932\u0947\u091c \u091c\u093e\u0928\u094d\u091b\u0941\", \"\u092e \u0915\u0932\u0947\u091c \u091c\u093e\u0928\u094d\u091b\u0941 \u0906\u091c\"),\n",
        "        (\"Append (Start)\", \"\u090a \u0918\u0930 \u0917\u092f\u094b\", \"\u0906\u091c \u090a \u0918\u0930 \u0917\u092f\u094b\"),\n",
        "        (\"Delete\", \"\u092f\u094b \u0915\u093f\u0924\u093e\u092c \u0927\u0947\u0930\u0948 \u0930\u093e\u092e\u094d\u0930\u094b \u091b\", \"\u092f\u094b \u0915\u093f\u0924\u093e\u092c \u0930\u093e\u092e\u094d\u0930\u094b \u091b\"),\n",
        "        (\"Replace\", \"\u092e\u0948\u0932\u0947 \u0915\u093e\u092e \u0917\u0930\u0947\", \"\u092e\u0948\u0932\u0947 \u0915\u093e\u092e \u0917\u0930\u0947\u0902\"),\n",
        "        (\"Neg_DR (No Merge)\", \"\u092f\u094b \u0930\u093e\u0924\u094b \u091f\u094b\u092a\u0940 \u0939\u094b\", \"\u092f\u094b \u0928\u0940\u0932\u094b \u091f\u094b\u092a\u0940 \u0939\u094b\"),\n",
        "    ]\n",
        "\n",
        "    emojis = {\n",
        "        vocabulary.KEEP_ID: \"\u2705\", vocabulary.DELETE_ID: \"\ud83d\uddd1\ufe0f\",\n",
        "        vocabulary.REPLACE_ID: \"\ud83d\udd04\", vocabulary.APPEND_ID: \"\u2795\",\n",
        "        vocabulary.SWAP_NEXT_ID: \"\u2194\ufe0f\", vocabulary.SWAP_PREV_ID: \"\u21a9\ufe0f\",\n",
        "        vocabulary.MERGE_NEXT_ID: \"\u2198\ufe0f\", vocabulary.MERGE_PREV_ID: \"\u2199\ufe0f\",\n",
        "        vocabulary.SPLIT_ID: \"\u2704\", vocabulary.UNKNOWN_ID: \"\u2753\",\n",
        "        -100: \"\u2b1b\" # Special token ID\n",
        "    }\n",
        "    \n",
        "    all_tests_passed = True\n",
        "    for case_type, incorrect, correct in test_cases_comprehensive:\n",
        "        print(f\"\\n--- Test Case: {case_type} ---\")\n",
        "        inc_words = incorrect.split()\n",
        "        cor_words = correct.split()\n",
        "        word_tags, stats = generate_word_level_tags(inc_words, cor_words, vocabulary)\n",
        "        \n",
        "        print(f\"  Input:    '{incorrect}'\")\n",
        "        print(f\"  Correct:  '{correct}'\")\n",
        "        print(f\"  Tags (Word-Level): \", end=\"\")\n",
        "        for i, (word, tag_id) in enumerate(zip(inc_words, word_tags)):\n",
        "            tag_name = vocabulary.get_tag_name(tag_id)\n",
        "            print(f\"[{emojis.get(tag_id, '\u2753')} {tag_name} '{word}'] \", end=\"\")\n",
        "        print(f\"\\n  Stats:    {stats}\")\n",
        "        \n",
        "        # --- ADDED: Token-Level Verification ---\n",
        "        print(f\"\\n  Token-Level Mapping (Verification):\")\n",
        "        \n",
        "        encoding = tokenizer(\n",
        "            incorrect,\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            truncation=True\n",
        "        )\n",
        "        subword_tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
        "        word_ids = encoding.word_ids()\n",
        "        \n",
        "        subword_labels = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                subword_labels.append(-100)\n",
        "            elif word_idx < len(word_tags):\n",
        "                if word_idx != previous_word_idx:\n",
        "                    subword_labels.append(word_tags[word_idx])\n",
        "                else:\n",
        "                    subword_labels.append(vocabulary.KEEP_ID)\n",
        "            else:\n",
        "                subword_labels.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "            \n",
        "        print(f\"    {'Token'.ljust(15)} {'Word ID'.ljust(10)} {'Final Label'.ljust(20)}\")\n",
        "        print(f\"    {'-'*15} {'-'*10} {'-'*20}\")\n",
        "        \n",
        "        for token, word_id, label_id in zip(subword_tokens, word_ids, subword_labels):\n",
        "            tag_name = vocabulary.get_tag_name(label_id) if label_id != -100 else \"N/A\"\n",
        "            emoji_tag = f\"{emojis.get(label_id, '\u2753')} {tag_name}\"\n",
        "            print(f\"    {token.ljust(15)} {str(word_id).ljust(10)} {emoji_tag.ljust(20)}\")\n",
        "        # --- END: Token-Level Verification ---\n",
        "\n",
        "        # Simple validation for key cases\n",
        "        if case_type == \"Merge\" and vocabulary.MERGE_NEXT_ID not in word_tags: all_tests_passed = False\n",
        "        if case_type == \"Split\" and vocabulary.SPLIT_ID not in word_tags: all_tests_passed = False\n",
        "        if case_type == \"Swap_Adj\" and vocabulary.SWAP_NEXT_ID not in word_tags: all_tests_passed = False\n",
        "        if case_type == \"Swap_Complex\" and vocabulary.SWAP_NEXT_ID not in word_tags: all_tests_passed = False\n",
        "        if case_type == \"Append (End)\" and vocabulary.APPEND_ID not in word_tags: all_tests_passed = False\n",
        "        if case_type == \"Append (Start)\" and vocabulary.APPEND_ID not in word_tags: all_tests_passed = False\n",
        "\n",
        "    if all_tests_passed:\n",
        "        print(\"\\n\u2713 All critical logic tests passed!\")\n",
        "    else:\n",
        "        print(\"\\n\u2717 CRITICAL ERROR: Tagging logic verification failed. Halting.\")\n",
        "        return\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 3: LOADING RAW DATASET\")\n",
        "    print(\"=\" * 70)\n",
        "    try:\n",
        "        raw_dataset = load_dataset(RAW_DATASET_NAME)\n",
        "        print(\"Raw dataset structure:\")\n",
        "        print(raw_dataset)\n",
        "        \n",
        "        combined_raw = concatenate_datasets([raw_dataset['train'], raw_dataset['valid']])\n",
        "        print(f\"Total raw examples to process: {len(combined_raw):,}\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Could not load raw dataset '{RAW_DATASET_NAME}'. {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"STEP 4: PROCESSING DATASET (Using {NUM_WORKERS} workers)\")\n",
        "    print(\"This will take several minutes...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Initialize worker globals for multiprocessing\n",
        "    # The globals 'tokenizer' and 'vocabulary' are set in STEP 1\n",
        "    # and will be inherited by the child processes.\n",
        "    processed_dataset = combined_raw.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        batch_size=2000, \n",
        "        num_proc=NUM_WORKERS,\n",
        "        remove_columns=combined_raw.column_names,\n",
        "        load_from_cache_file=False\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n\u2713 Dataset processing complete.\")\n",
        "    print(f\"  Total examples generated: {len(processed_dataset):,}\")\n",
        "    print(f\"  Time taken: {elapsed/60:.2f} minutes\")\n",
        "    print(f\"  New features: {processed_dataset.features}\")\n",
        "    \n",
        "    # --- ADDED: FIX FOR STRATIFIED SPLIT ---\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 4.5: CASTING STRATIFY KEY TO CLASSLABEL FOR STRATIFICATION\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # The labels are the string representations of the integer IDs (0 to 9)\n",
        "    class_names = [str(i) for i in range(vocabulary.vocab_size())]\n",
        "    processed_dataset = processed_dataset.cast_column(\n",
        "        \"stratify_key\", \n",
        "        datasets.ClassLabel(names=class_names)\n",
        "    )\n",
        "    print(\"\u2713 'stratify_key' successfully cast to ClassLabel type for stratification.\")\n",
        "    # --- END FIX ---\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 5: CREATING STRATIFIED SPLITS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    try:\n",
        "        temp_split = processed_dataset.train_test_split(\n",
        "            test_size=0.05,\n",
        "            stratify_by_column=\"stratify_key\",\n",
        "            seed=42\n",
        "        )\n",
        "        train_valid_set = temp_split['train']\n",
        "        calib_set = temp_split['test']\n",
        "\n",
        "        final_split = train_valid_set.train_test_split(\n",
        "            test_size=0.1579, # 0.15 / 0.95\n",
        "            stratify_by_column=\"stratify_key\",\n",
        "            seed=42\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Stratified split failed. {e}\")\n",
        "        print(\"Falling back to standard split...\")\n",
        "        temp_split = processed_dataset.train_test_split(test_size=0.05, seed=42)\n",
        "        train_valid_set = temp_split['train']\n",
        "        calib_set = temp_split['test']\n",
        "        final_split = train_valid_set.train_test_split(test_size=0.1579, seed=42)\n",
        "\n",
        "\n",
        "    final_dataset = DatasetDict({\n",
        "        'train': final_split['train'],\n",
        "        'validation': final_split['test'],\n",
        "        'test': calib_set\n",
        "    })\n",
        "\n",
        "    # Clean up the stratification column\n",
        "    final_dataset = final_dataset.remove_columns([\"stratify_key\"])\n",
        "\n",
        "    print(\"Final dataset splits:\")\n",
        "    print(final_dataset)\n",
        "    print(f\"  Train: {len(final_dataset['train']):,}\")\n",
        "    print(f\"  Valid: {len(final_dataset['validation']):,}\")\n",
        "    print(f\"  Calib: {len(final_dataset['test']):,}\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 6: SAVING DATASET LOCALLY\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    if os.path.exists(LOCAL_DATA_PATH):\n",
        "        print(f\"Warning: Deleting old local path: {LOCAL_DATA_PATH}\")\n",
        "        shutil.rmtree(LOCAL_DATA_PATH)\n",
        "        \n",
        "    final_dataset.save_to_disk(LOCAL_DATA_PATH)\n",
        "    vocabulary.save(os.path.join(LOCAL_DATA_PATH, VOCAB_FILENAME))\n",
        "    print(f\"\u2713 Final dataset and vocabulary saved to {LOCAL_DATA_PATH}\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 7: GENERATING README.MD DATASET CARD\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Generate the README.md content\n",
        "    readme_content = f\"\"\"---\n",
        "language:\n",
        "- ne\n",
        "license: mit\n",
        "task_categories:\n",
        "- token-classification\n",
        "tags:\n",
        "- grammatical-error-correction\n",
        "- gec\n",
        "- nepali\n",
        "- gector\n",
        "- sequence-tagging\n",
        "size_categories:\n",
        "- 1M<n<10M\n",
        "---\n",
        "\n",
        "# Nepali GEC (gector style) Token Tagging Dataset\n",
        "\n",
        "This is a processed version of the [sumitaryal/nepali_grammatical_error_correction](https://huggingface.co/datasets/sumitaryal/nepali_grammatical_error_correction) dataset,\n",
        "designed for training GEC-ToR-style sequence tagging models.\n",
        "\n",
        "This dataset has been processed with a robust, multi-pass, content-aware alignment algorithm\n",
        "to generate high-fidelity correction tags, **including complex and adjacent SWAP operations**.\n",
        "\n",
        "## Total Examples: {len(processed_dataset):,}\n",
        "\n",
        "* **Training:** {len(final_dataset['train']):,}\n",
        "* **Validation:** {len(final_dataset['validation']):,}\n",
        "* **test:** {len(final_dataset['test']):,}\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Token-Level Tags:** Word-level corrections are mapped to subword (token) labels.\n",
        "- **Correct Sentences Included:** The dataset contains both incorrect and correct sentences (tagged with `$KEEP`) for model stability.\n",
        "- **Stratified Splits:** Splits are stratified by the dominant error tag to ensure balanced evaluation.\n",
        "\n",
        "## Tag Vocabulary (10 Tags)\n",
        "\n",
        "This dataset uses an enhanced 10-tag system:\n",
        "\n",
        "1.  **`$KEEP`**: Token is correct.  -> label -> 0\n",
        "2.  **`$DELETE`**: Token should be deleted. -> label -> 1\n",
        "3.  **`$REPLACE`**: Token should be replaced (e.g., by a Transformer's MLM head). -> label -> 2\n",
        "4.  **`$APPEND`**: A new token should be inserted *after* this token. -> label -> 3\n",
        "5.  **`$SWAP_NEXT`**: Token is part of a swap (first word). -> label -> 4\n",
        "6.  **`$SWAP_PREV`**: Token is part of a swap (second word). -> label -> 5\n",
        "7.  **`$MERGE_NEXT`**: This token should be merged with the next token. -> label -> 6\n",
        "8.  **`$MERGE_PREV`**: This token should be merged with the previous token. -> label -> 7\n",
        "9.  **`$SPLIT`**: This token should be split into multiple tokens. -> label -> 8\n",
        "10. **`$UNKNOWN`**: Fallback tag (should not be present). -> label -> 9\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "REPO_ID = \"{REPO_ID}\"\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(REPO_ID)\n",
        "\n",
        "# Load vocabulary\n",
        "vocab_file = hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    filename=\"{VOCAB_FILENAME}\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "    vocabulary = json.load(f)\n",
        "\n",
        "print(f\"Splits: {{list(dataset.keys())}}\")\n",
        "print(f\"Tags: {{vocabulary['tag_to_id']}}\")\n",
        "```\n",
        "\"\"\"\n",
        "    \n",
        "    readme_path = os.path.join(LOCAL_DATA_PATH, \"README.md\")\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(readme_content)\n",
        "    print(f\"\u2713 README.md saved to {readme_path}\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 8: UPLOADING TO HUGGING FACE HUB\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    print(f\"Authenticating as {HF_USERNAME}...\")\n",
        "    try:\n",
        "        login(token=HF_TOKEN, add_to_git_credential=True)\n",
        "        print(\"\u2713 Authentication successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Authentication failed. {e}\")\n",
        "        print(\"Please check your HF_TOKEN.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Creating repository: {REPO_ID}\")\n",
        "    try:\n",
        "        create_repo(repo_id=REPO_ID, repo_type=\"dataset\", private=False, exist_ok=True)\n",
        "        print(f\"\u2713 Repository created or already exists.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Could not create repository. {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Uploading all files from {LOCAL_DATA_PATH}...\")\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        api.upload_folder(\n",
        "            folder_path=LOCAL_DATA_PATH,\n",
        "            repo_id=REPO_ID,\n",
        "            repo_type=\"dataset\",\n",
        "            commit_message=\"Uploading GEC dataset with 10 tags (incl. Swap/Merge/Split)\"\n",
        "        )\n",
        "        print(\"\\n\u2705 UPLOAD COMPLETE!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCRITICAL ERROR: Upload failed. {e}\")\n",
        "        print(\"Please check your permissions and internet connection.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 9: VERIFYING UPLOAD\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    try:\n",
        "        print(f\"Loading dataset from Hub: {REPO_ID}\")\n",
        "        time.sleep(5) # Give the hub a moment to update\n",
        "        verify_dataset = load_dataset(REPO_ID)\n",
        "        print(\"\u2713 Verification successful! Dataset structure on Hub:\")\n",
        "        print(verify_dataset)\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0 Verification failed. This might be a temporary delay. {e}\")\n",
        "        print(f\"Please check your dataset manually at: https://huggingface.co/datasets/{REPO_ID}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ALL STEPS COMPLETE.\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure multiprocessing works correctly with Hugging Face tokenizers\n",
        "    # This must be done *before* any other imports\n",
        "    datasets.utils.logging.set_verbosity_error()\n",
        "    \n",
        "    # Run the full pipeline\n",
        "    run_pipeline()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "STEP 1: INITIALIZING TOKENIZER & VOCABULARY\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1088e90c8cb54de2939e49525381089c",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7387d3d113eb4c5891ec46b8041a8bfe",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a92fa9afff14515ae5ff493a79bf253",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/968 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Tokenizer loaded: IRIIS-RESEARCH/RoBERTa_Nepali_125M\n",
            "\u2713 Vocabulary loaded: 10 tags\n",
            "Tags: ['$KEEP', '$DELETE', '$REPLACE', '$APPEND', '$SWAP_NEXT', '$SWAP_PREV', '$MERGE_NEXT', '$MERGE_PREV', '$SPLIT', '$UNKNOWN']\n",
            "\n",
            "======================================================================\n",
            "STEP 2: VERIFYING TAGGING LOGIC (COMPREHENSIVE TEST)\n",
            "======================================================================\n",
            "\n",
            "--- Test Case: Merge ---\n",
            "  Input:    '\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0901\u0926\u0948 \u091b\u0941'\n",
            "  Correct:  '\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0901\u0926\u0948\u091b\u0941'\n",
            "  Tags (Word-Level): [\u2705 $KEEP '\u092e'] [\u2705 $KEEP '\u0916\u093e\u0928\u093e'] [\u2198\ufe0f $MERGE_NEXT '\u0916\u093e\u0901\u0926\u0948'] [\u2199\ufe0f $MERGE_PREV '\u091b\u0941'] \n",
            "  Stats:    {'merge_next': 1, 'merge_prev': 1, 'keep': 2}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u092e              0          \u2705 $KEEP             \n",
            "    \u2581\u0916\u093e\u0928\u093e           1          \u2705 $KEEP             \n",
            "    \u2581\u0916\u093e\u0901\u0926\u0948          2          \u2198\ufe0f $MERGE_NEXT      \n",
            "    \u2581\u091b\u0941             3          \u2199\ufe0f $MERGE_PREV      \n",
            "\n",
            "--- Test Case: Split ---\n",
            "  Input:    '\u0924\u092a\u093e\u0908\u0902\u0915\u094b \u0928\u093e\u092e\u0915\u0947 \u0939\u094b'\n",
            "  Correct:  '\u0924\u092a\u093e\u0908\u0902\u0915\u094b \u0928\u093e\u092e \u0915\u0947 \u0939\u094b'\n",
            "  Tags (Word-Level): [\u2705 $KEEP '\u0924\u092a\u093e\u0908\u0902\u0915\u094b'] [\u2704 $SPLIT '\u0928\u093e\u092e\u0915\u0947'] [\u2705 $KEEP '\u0939\u094b'] \n",
            "  Stats:    {'split': 1, 'keep': 2}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u0924\u092a\u093e\u0908\u0902\u0915\u094b        0          \u2705 $KEEP             \n",
            "    \u2581\u0928\u093e\u092e            1          \u2704 $SPLIT            \n",
            "    \u0915\u0947              1          \u2705 $KEEP             \n",
            "    \u2581\u0939\u094b             2          \u2705 $KEEP             \n",
            "\n",
            "--- Test Case: Swap_Adj ---\n",
            "  Input:    '\u0916\u093e\u0928\u093e \u092e \u0916\u093e\u0928\u094d\u091b\u0941'\n",
            "  Correct:  '\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0928\u094d\u091b\u0941'\n",
            "  Tags (Word-Level): [\u2194\ufe0f $SWAP_NEXT '\u0916\u093e\u0928\u093e'] [\u21a9\ufe0f $SWAP_PREV '\u092e'] [\u2705 $KEEP '\u0916\u093e\u0928\u094d\u091b\u0941'] \n",
            "  Stats:    {'swap_next': 1, 'swap_prev': 1, 'keep': 1}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u0916\u093e\u0928\u093e           0          \u2194\ufe0f $SWAP_NEXT       \n",
            "    \u2581\u092e              1          \u21a9\ufe0f $SWAP_PREV       \n",
            "    \u2581\u0916\u093e             2          \u2705 $KEEP             \n",
            "    \u0928\u094d\u091b\u0941            2          \u2705 $KEEP             \n",
            "\n",
            "--- Test Case: Swap_Complex ---\n",
            "  Input:    '\u0938\u094b\u0927\u094d\u0928 \u0915\u0947\u0939\u0940 \u091b \u092e\u0932\u093e\u0908'\n",
            "  Correct:  '\u092e\u0932\u093e\u0908 \u0938\u094b\u0927\u094d\u0928 \u0915\u0947\u0939\u0940 \u091b'\n",
            "  Tags (Word-Level): [\u2194\ufe0f $SWAP_NEXT '\u0938\u094b\u0927\u094d\u0928'] [\u2705 $KEEP '\u0915\u0947\u0939\u0940'] [\u2705 $KEEP '\u091b'] [\u21a9\ufe0f $SWAP_PREV '\u092e\u0932\u093e\u0908'] \n",
            "  Stats:    {'swap_next': 1, 'swap_prev': 1, 'keep': 2}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u0938\u094b\u0927\u094d\u0928          0          \u2194\ufe0f $SWAP_NEXT       \n",
            "    \u2581\u0915\u0947\u0939\u0940           1          \u2705 $KEEP             \n",
            "    \u2581\u091b              2          \u2705 $KEEP             \n",
            "    \u2581\u092e\u0932\u093e\u0908           3          \u21a9\ufe0f $SWAP_PREV       \n",
            "\n",
            "--- Test Case: Append (End) ---\n",
            "  Input:    '\u092e \u0915\u0932\u0947\u091c \u091c\u093e\u0928\u094d\u091b\u0941'\n",
            "  Correct:  '\u092e \u0915\u0932\u0947\u091c \u091c\u093e\u0928\u094d\u091b\u0941 \u0906\u091c'\n",
            "  Tags (Word-Level): [\u2705 $KEEP '\u092e'] [\u2705 $KEEP '\u0915\u0932\u0947\u091c'] [\u2795 $APPEND '\u091c\u093e\u0928\u094d\u091b\u0941'] \n",
            "  Stats:    {'append': 1, 'keep': 2}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u092e              0          \u2705 $KEEP             \n",
            "    \u2581\u0915\u0932\u0947\u091c           1          \u2705 $KEEP             \n",
            "    \u2581\u091c\u093e\u0928\u094d\u091b\u0941         2          \u2795 $APPEND           \n",
            "\n",
            "--- Test Case: Append (Start) ---\n",
            "  Input:    '\u090a \u0918\u0930 \u0917\u092f\u094b'\n",
            "  Correct:  '\u0906\u091c \u090a \u0918\u0930 \u0917\u092f\u094b'\n",
            "  Tags (Word-Level): [\u2795 $APPEND '\u090a'] [\u2705 $KEEP '\u0918\u0930'] [\u2705 $KEEP '\u0917\u092f\u094b'] \n",
            "  Stats:    {'append': 1, 'keep': 2}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u090a              0          \u2795 $APPEND           \n",
            "    \u2581\u0918\u0930             1          \u2705 $KEEP             \n",
            "    \u2581\u0917\u092f\u094b            2          \u2705 $KEEP             \n",
            "\n",
            "--- Test Case: Delete ---\n",
            "  Input:    '\u092f\u094b \u0915\u093f\u0924\u093e\u092c \u0927\u0947\u0930\u0948 \u0930\u093e\u092e\u094d\u0930\u094b \u091b'\n",
            "  Correct:  '\u092f\u094b \u0915\u093f\u0924\u093e\u092c \u0930\u093e\u092e\u094d\u0930\u094b \u091b'\n",
            "  Tags (Word-Level): [\u2705 $KEEP '\u092f\u094b'] [\u2705 $KEEP '\u0915\u093f\u0924\u093e\u092c'] [\ud83d\uddd1\ufe0f $DELETE '\u0927\u0947\u0930\u0948'] [\u2705 $KEEP '\u0930\u093e\u092e\u094d\u0930\u094b'] [\u2705 $KEEP '\u091b'] \n",
            "  Stats:    {'delete': 1, 'keep': 4}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u092f\u094b             0          \u2705 $KEEP             \n",
            "    \u2581\u0915\u093f\u0924\u093e\u092c          1          \u2705 $KEEP             \n",
            "    \u2581\u0927\u0947\u0930\u0948           2          \ud83d\uddd1\ufe0f $DELETE          \n",
            "    \u2581\u0930\u093e\u092e\u094d\u0930\u094b         3          \u2705 $KEEP             \n",
            "    \u2581\u091b              4          \u2705 $KEEP             \n",
            "\n",
            "--- Test Case: Replace ---\n",
            "  Input:    '\u092e\u0948\u0932\u0947 \u0915\u093e\u092e \u0917\u0930\u0947'\n",
            "  Correct:  '\u092e\u0948\u0932\u0947 \u0915\u093e\u092e \u0917\u0930\u0947\u0902'\n",
            "  Tags (Word-Level): [\u2705 $KEEP '\u092e\u0948\u0932\u0947'] [\u2705 $KEEP '\u0915\u093e\u092e'] [\ud83d\udd04 $REPLACE '\u0917\u0930\u0947'] \n",
            "  Stats:    {'replace': 1, 'keep': 2}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u092e\u0948\u0932\u0947           0          \u2705 $KEEP             \n",
            "    \u2581\u0915\u093e\u092e            1          \u2705 $KEEP             \n",
            "    \u2581\u0917\u0930\u0947            2          \ud83d\udd04 $REPLACE          \n",
            "\n",
            "--- Test Case: Neg_DR (No Merge) ---\n",
            "  Input:    '\u092f\u094b \u0930\u093e\u0924\u094b \u091f\u094b\u092a\u0940 \u0939\u094b'\n",
            "  Correct:  '\u092f\u094b \u0928\u0940\u0932\u094b \u091f\u094b\u092a\u0940 \u0939\u094b'\n",
            "  Tags (Word-Level): [\u2705 $KEEP '\u092f\u094b'] [\ud83d\udd04 $REPLACE '\u0930\u093e\u0924\u094b'] [\u2705 $KEEP '\u091f\u094b\u092a\u0940'] [\u2705 $KEEP '\u0939\u094b'] \n",
            "  Stats:    {'replace': 1, 'keep': 3}\n",
            "\n",
            "  Token-Level Mapping (Verification):\n",
            "    Token           Word ID    Final Label         \n",
            "    --------------- ---------- --------------------\n",
            "    \u2581\u092f\u094b             0          \u2705 $KEEP             \n",
            "    \u2581\u0930\u093e\u0924\u094b           1          \ud83d\udd04 $REPLACE          \n",
            "    \u2581\u091f\u094b\u092a\u0940           2          \u2705 $KEEP             \n",
            "    \u2581\u0939\u094b             3          \u2705 $KEEP             \n",
            "\n",
            "\u2713 All critical logic tests passed!\n",
            "\n",
            "======================================================================\n",
            "STEP 3: LOADING RAW DATASET\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a6b3ffae5654472b3602bc228d74947",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "README.md:   0%|          | 0.00/451 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61c592460263400291b0965dab5de017",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/train-00000-of-00007.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f53ecd8dc755459abbfbad993e17dac5",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/train-00001-of-00007.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a02aeb98f559452c94096b6c1551dc0a",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/train-00002-of-00007.parquet:   0%|          | 0.00/216M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c410d95514c9439486e0303b55db9f9e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/train-00003-of-00007.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "921e0306bf5747e6ad174468b2598d69",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/train-00004-of-00007.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28c37693f8574c56a44977d05c75c397",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/train-00005-of-00007.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ecf1371da3b45e183e96ed650ca63d7",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/train-00006-of-00007.parquet:   0%|          | 0.00/217M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f7667329046462ba4167721e51193d7",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "data/valid-00000-of-00001.parquet:   0%|          | 0.00/79.9M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3af4c2ef80894c08b49257538249c2e7",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Generating train split:   0%|          | 0/7723971 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e52a5b41976b43a8ab576bd0b1f2cc58",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Generating valid split:   0%|          | 0/406525 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['incorrect_sentence', 'correct_sentence'],\n",
            "        num_rows: 7723971\n",
            "    })\n",
            "    valid: Dataset({\n",
            "        features: ['incorrect_sentence', 'correct_sentence'],\n",
            "        num_rows: 406525\n",
            "    })\n",
            "})\n",
            "Total raw examples to process: 8,130,496\n",
            "\n",
            "======================================================================\n",
            "STEP 4: PROCESSING DATASET (Using 106 workers)\n",
            "This will take several minutes...\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa6f4d4bd89246e087a222e701c697a5",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Map (num_proc=106):   0%|          | 0/8130496 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2713 Dataset processing complete.\n",
            "  Total examples generated: 16,260,992\n",
            "  Time taken: 1.65 minutes\n",
            "  New features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64')), 'is_correct': Value('bool'), 'tag_stats': Value('string'), 'stratify_key': Value('string')}\n",
            "\n",
            "======================================================================\n",
            "STEP 4.5: CASTING STRATIFY KEY TO CLASSLABEL FOR STRATIFICATION\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bae74d50c07642189e8b86475fb4afa5",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Casting the dataset:   0%|          | 0/16260992 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 'stratify_key' successfully cast to ClassLabel type for stratification.\n",
            "\n",
            "======================================================================\n",
            "STEP 5: CREATING STRATIFIED SPLITS\n",
            "======================================================================\n",
            "Final dataset splits:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
            "        num_rows: 13008711\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
            "        num_rows: 2439231\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
            "        num_rows: 813050\n",
            "    })\n",
            "})\n",
            "  Train: 13,008,711\n",
            "  Valid: 2,439,231\n",
            "  Calib: 813,050\n",
            "\n",
            "======================================================================\n",
            "STEP 6: SAVING DATASET LOCALLY\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f9709fab76046cb8e94b439a5634b62",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Saving the dataset (0/45 shards):   0%|          | 0/13008711 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82199bdc1e184790944a138e9196be66",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Saving the dataset (0/9 shards):   0%|          | 0/2439231 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36b9704357f545e890a42c045098cb81",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Saving the dataset (0/3 shards):   0%|          | 0/813050 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token has not been saved to git credential helper.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Vocabulary saved: ./nepali-gector-style-token-level-tag-for-ged/gec_vocabulary.json\n",
            "\u2713 Final dataset and vocabulary saved to ./nepali-gector-style-token-level-tag-for-ged\n",
            "\n",
            "======================================================================\n",
            "STEP 7: GENERATING README.MD DATASET CARD\n",
            "======================================================================\n",
            "\u2713 README.md saved to ./nepali-gector-style-token-level-tag-for-ged/README.md\n",
            "\n",
            "======================================================================\n",
            "STEP 8: UPLOADING TO HUGGING FACE HUB\n",
            "======================================================================\n",
            "Authenticating as DipeshChaudhary...\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "\u2713 Authentication successful.\n",
            "Creating repository: DipeshChaudhary/nepali-gector-style-token-level-tag-for-ged\n",
            "\u2713 Repository created or already exists.\n",
            "Uploading all files from ./nepali-gector-style-token-level-tag-for-ged...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08b29165292a482f83f65cc3be284084",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "68115ef295734d02b9c60e3065341962",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "New Data Upload                         : |          |  0.00B /  0.00B            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5923242901e142ea9b0135a6f94df26d",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...-ged/test/data-00000-of-00003.arrow:   7%|7         | 33.5MB /  462MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c30189cdcbd477a8f5b4d605cb450dc",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...ged/train/data-00000-of-00045.arrow:   5%|5         | 25.1MB /  492MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52c000ece8534ac388f724d37beeb425",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...-ged/test/data-00001-of-00003.arrow:   5%|5         | 25.0MB /  462MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c61630457b754bc5babd2f4bb00a5c27",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...ged/train/data-00001-of-00045.arrow:   5%|5         | 25.0MB /  492MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b5c38e2da7344699e8c8ee94305286c",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...-ged/test/data-00002-of-00003.arrow:   7%|7         | 33.5MB /  462MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f72ca5681b34482962b10cdb09a3a49",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...ged/train/data-00006-of-00045.arrow:   5%|5         | 25.1MB /  492MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba7917707d084ae180dcd04090ffeea2",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...ged/train/data-00004-of-00045.arrow:   5%|5         | 25.1MB /  492MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21cd7bed18f54ae091c69801b24b29bb",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...ged/train/data-00009-of-00045.arrow:   7%|6         | 33.5MB /  492MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "446a3c461ea44632a1caa031c742da62",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...ged/train/data-00011-of-00045.arrow:   2%|1         | 8.31MB /  492MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22fcd3a745d94b92b90375b1e87e9300",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "  ...ged/train/data-00010-of-00045.arrow:   2%|1         | 8.29MB /  492MB            "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 UPLOAD COMPLETE!\n",
            "\n",
            "======================================================================\n",
            "STEP 9: VERIFYING UPLOAD\n",
            "======================================================================\n",
            "Loading dataset from Hub: DipeshChaudhary/nepali-gector-style-token-level-tag-for-ged\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06fce1f04d1f4d788e4a0bb075569dbd",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "README.md: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "033ab15e93304bb3a57f380f14d22b48",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Resolving data files:   0%|          | 0/46 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3aca112f90894a70a0fec0885a783fc1",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Downloading data:   0%|          | 0/45 [00:00<?, ?files/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "024e729ffd3e4165b757ca30b0d93fd0",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00000-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "500316e302d043fbac559bde0ace2762",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00001-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24eabff3f3cf458cbc1087e46c92991e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00002-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "896d909badc24e1eafb1cf088e964b9c",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00003-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fd9ad0fdccf4cf3b17516cbee4787a4",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00004-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0d5cfe719164424aa0ddbf62fa88859",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00005-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d09eb2918c840d181c80e0096d11237",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00006-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "639c976b84bb4036bc6f10601b65f324",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00007-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c203106255d94c20ae8fc74fabb5b65f",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00008-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8dc02b4e481412cb6138c9509ad5389",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00009-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "beeb5d569a624d63861e59a6958d0ffd",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00010-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "062c1c5f956d4306832ec48ef6b270ae",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00011-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f17f760ba1e4481ba97378e79772ab50",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00012-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "555db1771f054d9494b9cc731bfd7fd8",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00013-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cd85aaa9b8e462f814115bbda52c744",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00014-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b33baae17f3d4989841daf6f0ae12f8e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00015-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdd241b548444041b1cfdf01611e8d65",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00016-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b01c812a119f4ae4b01ffade57b8356e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00017-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "073dfa1905c54f158f28a6ecd469d55d",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00018-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e29db2c5b9841258b15a59d0d274c98",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00019-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd731b1f1de34e1fbdf249e77681daaf",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00020-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "091cb5c7a5ba43449fb6f0528b7b0076",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00021-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "489467f3a44e4674b6182ef70d31f970",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00022-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e009f04930c4657a60ad94784928b83",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00023-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85b9986578c24ac7a18758ecf1e49c1e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00024-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcfbbf1450814f838529bd434f80cc50",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00025-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5066fdbb0d14db5b528017594f9189a",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00026-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b41e78d4c33747dc80a899b1ce742b53",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00027-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4b615dab2e74ca99684274e0dfc348a",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00028-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4299a036c0f426d9a0cfb19ec031ba1",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00029-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa7f30d762ae4a918d7425ec1714aabe",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00030-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4ed7a829d0642d598d68ea8aa2a05fc",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00031-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ec95cedc1af402ba440d5bc0d1cf1ce",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00032-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ab0047427dc48a79fc8cf5d2391c5ec",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00033-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c9abec909894370872691c212fdd483",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00034-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3f3a08147c34f9d8c766dcf8d060584",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00035-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "878ed515e6304766874c3de4a1f60811",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00036-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bfb20e6fff04aceb9e9705f06b6c9e2",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00037-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08530b40988d49718abeeab063e544b4",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00038-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c524e04c79194ed3a96a3b3d0365ce7c",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00039-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54cc2c19905f42528de286daae616c9f",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00040-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d811555923b24f9ab05f07fedf2b8b1a",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00041-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e82975120ee4101904b29155976e9db",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00042-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2af6ac94a5fc41dbb51b2ae0e742e4c1",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00043-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34c12737ceae4d268ff031798da67fbc",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "train/data-00044-of-00045.arrow:   0%|          | 0.00/492M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f8a1061938342ea9b577340437b6037",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00000-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a61e76c899ee468ab72bfe21d52bb4f0",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00001-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "22bcbc7f0d284decbeac5882453f5b95",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00002-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "755541a2565145878b56893d72c79c1c",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00003-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ed6713c0be644018cd86db2bdb33e49",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00004-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee18c660b2c6467f872847129549a6a1",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00005-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9ba0cf7520e4653a15e822b0b9b8c51",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00006-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee503c654b1b493fbe548317965ac4e1",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00007-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71e2d83d3f7049589d32fac3b0549ce0",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "validation/data-00008-of-00009.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa97532878254bc3b81b237d9af9d8f2",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "test/data-00000-of-00003.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75ba188c68db4fd18fa2f0c3c32d5ba3",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "test/data-00001-of-00003.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb83dc6d7d834a32bce9fcccc2a002bb",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "test/data-00002-of-00003.arrow:   0%|          | 0.00/462M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e65e2de2719b484495606c8ac6813f70",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7227b9dd470e4ae4aeaba9a863d0e0bd",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f249dd84193b4beebc06b783a63b345e",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29ecdc74ead8411fb13e70c5ed9856e3",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Loading dataset shards:   0%|          | 0/45 [00:00<?, ?it/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Verification successful! Dataset structure on Hub:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
            "        num_rows: 13008711\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
            "        num_rows: 2439231\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
            "        num_rows: 813050\n",
            "    })\n",
            "})\n",
            "\n",
            "======================================================================\n",
            "ALL STEPS COMPLETE.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# output of the above cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "======================================================================\n",
        "STEP 1: INITIALIZING TOKENIZER & VOCABULARY\n",
        "======================================================================\n",
        "tokenizer_config.json:\u2007\n",
        "\u20071.62k/?\u2007[00:00<00:00,\u200789.2kB/s]\n",
        "tokenizer.json:\u2007\n",
        "\u20074.89M/?\u2007[00:00<00:00,\u200716.0MB/s]\n",
        "special_tokens_map.json:\u2007100%\n",
        "\u2007968/968\u2007[00:00<00:00,\u200782.8kB/s]\n",
        "\u2713 Tokenizer loaded: IRIIS-RESEARCH/RoBERTa_Nepali_125M\n",
        "\u2713 Vocabulary loaded: 10 tags\n",
        "Tags: ['$KEEP', '$DELETE', '$REPLACE', '$APPEND', '$SWAP_NEXT', '$SWAP_PREV', '$MERGE_NEXT', '$MERGE_PREV', '$SPLIT', '$UNKNOWN']\n",
        "\n",
        "======================================================================\n",
        "STEP 2: VERIFYING TAGGING LOGIC (COMPREHENSIVE TEST)\n",
        "======================================================================\n",
        "\n",
        "--- Test Case: Merge ---\n",
        "  Input:    '\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0901\u0926\u0948 \u091b\u0941'\n",
        "  Correct:  '\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0901\u0926\u0948\u091b\u0941'\n",
        "  Tags (Word-Level): [\u2705 $KEEP '\u092e'] [\u2705 $KEEP '\u0916\u093e\u0928\u093e'] [\u2198\ufe0f $MERGE_NEXT '\u0916\u093e\u0901\u0926\u0948'] [\u2199\ufe0f $MERGE_PREV '\u091b\u0941'] \n",
        "  Stats:    {'merge_next': 1, 'merge_prev': 1, 'keep': 2}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u092e              0          \u2705 $KEEP             \n",
        "    \u2581\u0916\u093e\u0928\u093e           1          \u2705 $KEEP             \n",
        "    \u2581\u0916\u093e\u0901\u0926\u0948          2          \u2198\ufe0f $MERGE_NEXT      \n",
        "    \u2581\u091b\u0941             3          \u2199\ufe0f $MERGE_PREV      \n",
        "\n",
        "--- Test Case: Split ---\n",
        "  Input:    '\u0924\u092a\u093e\u0908\u0902\u0915\u094b \u0928\u093e\u092e\u0915\u0947 \u0939\u094b'\n",
        "  Correct:  '\u0924\u092a\u093e\u0908\u0902\u0915\u094b \u0928\u093e\u092e \u0915\u0947 \u0939\u094b'\n",
        "  Tags (Word-Level): [\u2705 $KEEP '\u0924\u092a\u093e\u0908\u0902\u0915\u094b'] [\u2704 $SPLIT '\u0928\u093e\u092e\u0915\u0947'] [\u2705 $KEEP '\u0939\u094b'] \n",
        "  Stats:    {'split': 1, 'keep': 2}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u0924\u092a\u093e\u0908\u0902\u0915\u094b        0          \u2705 $KEEP             \n",
        "    \u2581\u0928\u093e\u092e            1          \u2704 $SPLIT            \n",
        "    \u0915\u0947              1          \u2705 $KEEP             \n",
        "    \u2581\u0939\u094b             2          \u2705 $KEEP             \n",
        "\n",
        "--- Test Case: Swap_Adj ---\n",
        "  Input:    '\u0916\u093e\u0928\u093e \u092e \u0916\u093e\u0928\u094d\u091b\u0941'\n",
        "  Correct:  '\u092e \u0916\u093e\u0928\u093e \u0916\u093e\u0928\u094d\u091b\u0941'\n",
        "  Tags (Word-Level): [\u2194\ufe0f $SWAP_NEXT '\u0916\u093e\u0928\u093e'] [\u21a9\ufe0f $SWAP_PREV '\u092e'] [\u2705 $KEEP '\u0916\u093e\u0928\u094d\u091b\u0941'] \n",
        "  Stats:    {'swap_next': 1, 'swap_prev': 1, 'keep': 1}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u0916\u093e\u0928\u093e           0          \u2194\ufe0f $SWAP_NEXT       \n",
        "    \u2581\u092e              1          \u21a9\ufe0f $SWAP_PREV       \n",
        "    \u2581\u0916\u093e             2          \u2705 $KEEP             \n",
        "    \u0928\u094d\u091b\u0941            2          \u2705 $KEEP             \n",
        "\n",
        "--- Test Case: Swap_Complex ---\n",
        "  Input:    '\u0938\u094b\u0927\u094d\u0928 \u0915\u0947\u0939\u0940 \u091b \u092e\u0932\u093e\u0908'\n",
        "  Correct:  '\u092e\u0932\u093e\u0908 \u0938\u094b\u0927\u094d\u0928 \u0915\u0947\u0939\u0940 \u091b'\n",
        "  Tags (Word-Level): [\u2194\ufe0f $SWAP_NEXT '\u0938\u094b\u0927\u094d\u0928'] [\u2705 $KEEP '\u0915\u0947\u0939\u0940'] [\u2705 $KEEP '\u091b'] [\u21a9\ufe0f $SWAP_PREV '\u092e\u0932\u093e\u0908'] \n",
        "  Stats:    {'swap_next': 1, 'swap_prev': 1, 'keep': 2}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u0938\u094b\u0927\u094d\u0928          0          \u2194\ufe0f $SWAP_NEXT       \n",
        "    \u2581\u0915\u0947\u0939\u0940           1          \u2705 $KEEP             \n",
        "    \u2581\u091b              2          \u2705 $KEEP             \n",
        "    \u2581\u092e\u0932\u093e\u0908           3          \u21a9\ufe0f $SWAP_PREV       \n",
        "\n",
        "--- Test Case: Append (End) ---\n",
        "  Input:    '\u092e \u0915\u0932\u0947\u091c \u091c\u093e\u0928\u094d\u091b\u0941'\n",
        "  Correct:  '\u092e \u0915\u0932\u0947\u091c \u091c\u093e\u0928\u094d\u091b\u0941 \u0906\u091c'\n",
        "  Tags (Word-Level): [\u2705 $KEEP '\u092e'] [\u2705 $KEEP '\u0915\u0932\u0947\u091c'] [\u2795 $APPEND '\u091c\u093e\u0928\u094d\u091b\u0941'] \n",
        "  Stats:    {'append': 1, 'keep': 2}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u092e              0          \u2705 $KEEP             \n",
        "    \u2581\u0915\u0932\u0947\u091c           1          \u2705 $KEEP             \n",
        "    \u2581\u091c\u093e\u0928\u094d\u091b\u0941         2          \u2795 $APPEND           \n",
        "\n",
        "--- Test Case: Append (Start) ---\n",
        "  Input:    '\u090a \u0918\u0930 \u0917\u092f\u094b'\n",
        "  Correct:  '\u0906\u091c \u090a \u0918\u0930 \u0917\u092f\u094b'\n",
        "  Tags (Word-Level): [\u2795 $APPEND '\u090a'] [\u2705 $KEEP '\u0918\u0930'] [\u2705 $KEEP '\u0917\u092f\u094b'] \n",
        "  Stats:    {'append': 1, 'keep': 2}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u090a              0          \u2795 $APPEND           \n",
        "    \u2581\u0918\u0930             1          \u2705 $KEEP             \n",
        "    \u2581\u0917\u092f\u094b            2          \u2705 $KEEP             \n",
        "\n",
        "--- Test Case: Delete ---\n",
        "  Input:    '\u092f\u094b \u0915\u093f\u0924\u093e\u092c \u0927\u0947\u0930\u0948 \u0930\u093e\u092e\u094d\u0930\u094b \u091b'\n",
        "  Correct:  '\u092f\u094b \u0915\u093f\u0924\u093e\u092c \u0930\u093e\u092e\u094d\u0930\u094b \u091b'\n",
        "  Tags (Word-Level): [\u2705 $KEEP '\u092f\u094b'] [\u2705 $KEEP '\u0915\u093f\u0924\u093e\u092c'] [\ud83d\uddd1\ufe0f $DELETE '\u0927\u0947\u0930\u0948'] [\u2705 $KEEP '\u0930\u093e\u092e\u094d\u0930\u094b'] [\u2705 $KEEP '\u091b'] \n",
        "  Stats:    {'delete': 1, 'keep': 4}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u092f\u094b             0          \u2705 $KEEP             \n",
        "    \u2581\u0915\u093f\u0924\u093e\u092c          1          \u2705 $KEEP             \n",
        "    \u2581\u0927\u0947\u0930\u0948           2          \ud83d\uddd1\ufe0f $DELETE          \n",
        "    \u2581\u0930\u093e\u092e\u094d\u0930\u094b         3          \u2705 $KEEP             \n",
        "    \u2581\u091b              4          \u2705 $KEEP             \n",
        "\n",
        "--- Test Case: Replace ---\n",
        "  Input:    '\u092e\u0948\u0932\u0947 \u0915\u093e\u092e \u0917\u0930\u0947'\n",
        "  Correct:  '\u092e\u0948\u0932\u0947 \u0915\u093e\u092e \u0917\u0930\u0947\u0902'\n",
        "  Tags (Word-Level): [\u2705 $KEEP '\u092e\u0948\u0932\u0947'] [\u2705 $KEEP '\u0915\u093e\u092e'] [\ud83d\udd04 $REPLACE '\u0917\u0930\u0947'] \n",
        "  Stats:    {'replace': 1, 'keep': 2}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u092e\u0948\u0932\u0947           0          \u2705 $KEEP             \n",
        "    \u2581\u0915\u093e\u092e            1          \u2705 $KEEP             \n",
        "    \u2581\u0917\u0930\u0947            2          \ud83d\udd04 $REPLACE          \n",
        "\n",
        "--- Test Case: Neg_DR (No Merge) ---\n",
        "  Input:    '\u092f\u094b \u0930\u093e\u0924\u094b \u091f\u094b\u092a\u0940 \u0939\u094b'\n",
        "  Correct:  '\u092f\u094b \u0928\u0940\u0932\u094b \u091f\u094b\u092a\u0940 \u0939\u094b'\n",
        "  Tags (Word-Level): [\u2705 $KEEP '\u092f\u094b'] [\ud83d\udd04 $REPLACE '\u0930\u093e\u0924\u094b'] [\u2705 $KEEP '\u091f\u094b\u092a\u0940'] [\u2705 $KEEP '\u0939\u094b'] \n",
        "  Stats:    {'replace': 1, 'keep': 3}\n",
        "\n",
        "  Token-Level Mapping (Verification):\n",
        "    Token           Word ID    Final Label         \n",
        "    --------------- ---------- --------------------\n",
        "    \u2581\u092f\u094b             0          \u2705 $KEEP             \n",
        "    \u2581\u0930\u093e\u0924\u094b           1          \ud83d\udd04 $REPLACE          \n",
        "    \u2581\u091f\u094b\u092a\u0940           2          \u2705 $KEEP             \n",
        "    \u2581\u0939\u094b             3          \u2705 $KEEP             \n",
        "\n",
        "\u2713 All critical logic tests passed!\n",
        "\n",
        "======================================================================\n",
        "STEP 3: LOADING RAW DATASET\n",
        "======================================================================\n",
        "README.md:\u2007100%\n",
        "\u2007451/451\u2007[00:00<00:00,\u200741.7kB/s]\n",
        "data/train-00000-of-00007.parquet:\u2007100%\n",
        "\u2007217M/217M\u2007[00:01<00:00,\u2007171MB/s]\n",
        "data/train-00001-of-00007.parquet:\u2007100%\n",
        "\u2007217M/217M\u2007[00:02<00:00,\u2007147MB/s]\n",
        "data/train-00002-of-00007.parquet:\u2007100%\n",
        "\u2007216M/216M\u2007[00:01<00:00,\u2007182MB/s]\n",
        "data/train-00003-of-00007.parquet:\u2007100%\n",
        "\u2007217M/217M\u2007[00:01<00:00,\u2007123MB/s]\n",
        "data/train-00004-of-00007.parquet:\u2007100%\n",
        "\u2007217M/217M\u2007[00:02<00:00,\u2007130MB/s]\n",
        "data/train-00005-of-00007.parquet:\u2007100%\n",
        "\u2007217M/217M\u2007[00:02<00:00,\u2007140MB/s]\n",
        "data/train-00006-of-00007.parquet:\u2007100%\n",
        "\u2007217M/217M\u2007[00:02<00:00,\u2007130MB/s]\n",
        "data/valid-00000-of-00001.parquet:\u2007100%\n",
        "\u200779.9M/79.9M\u2007[00:01<00:00,\u200758.9MB/s]\n",
        "Generating\u2007train\u2007split:\u2007100%\n",
        "\u20077723971/7723971\u2007[00:10<00:00,\u2007897164.18\u2007examples/s]\n",
        "Generating\u2007valid\u2007split:\u2007100%\n",
        "\u2007406525/406525\u2007[00:00<00:00,\u2007792514.21\u2007examples/s]\n",
        "Raw dataset structure:\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['incorrect_sentence', 'correct_sentence'],\n",
        "        num_rows: 7723971\n",
        "    })\n",
        "    valid: Dataset({\n",
        "        features: ['incorrect_sentence', 'correct_sentence'],\n",
        "        num_rows: 406525\n",
        "    })\n",
        "})\n",
        "Total raw examples to process: 8,130,496\n",
        "\n",
        "======================================================================\n",
        "STEP 4: PROCESSING DATASET (Using 106 workers)\n",
        "This will take several minutes...\n",
        "======================================================================\n",
        "Map\u2007(num_proc=106):\u2007100%\n",
        "\u20078130496/8130496\u2007[01:37<00:00,\u200712988.00\u2007examples/s]\n",
        "\n",
        "\u2713 Dataset processing complete.\n",
        "  Total examples generated: 16,260,992\n",
        "  Time taken: 1.65 minutes\n",
        "  New features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64')), 'is_correct': Value('bool'), 'tag_stats': Value('string'), 'stratify_key': Value('string')}\n",
        "\n",
        "======================================================================\n",
        "STEP 4.5: CASTING STRATIFY KEY TO CLASSLABEL FOR STRATIFICATION\n",
        "======================================================================\n",
        "Casting\u2007the\u2007dataset:\u2007100%\n",
        "\u200716260992/16260992\u2007[01:39<00:00,\u2007141139.91\u2007examples/s]\n",
        "\u2713 'stratify_key' successfully cast to ClassLabel type for stratification.\n",
        "\n",
        "======================================================================\n",
        "STEP 5: CREATING STRATIFIED SPLITS\n",
        "======================================================================\n",
        "Final dataset splits:\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
        "        num_rows: 13008711\n",
        "    })\n",
        "    validation: Dataset({\n",
        "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
        "        num_rows: 2439231\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
        "        num_rows: 813050\n",
        "    })\n",
        "})\n",
        "  Train: 13,008,711\n",
        "  Valid: 2,439,231\n",
        "  Calib: 813,050\n",
        "\n",
        "======================================================================\n",
        "STEP 6: SAVING DATASET LOCALLY\n",
        "======================================================================\n",
        "Saving\u2007the\u2007dataset\u2007(45/45\u2007shards):\u2007100%\n",
        "\u200713008711/13008711\u2007[04:14<00:00,\u200757533.50\u2007examples/s]\n",
        "Saving\u2007the\u2007dataset\u2007(9/9\u2007shards):\u2007100%\n",
        "\u20072439231/2439231\u2007[00:45<00:00,\u200755115.58\u2007examples/s]\n",
        "Saving\u2007the\u2007dataset\u2007(3/3\u2007shards):\u2007100%\n",
        "\u2007813050/813050\u2007[00:14<00:00,\u200752051.22\u2007examples/s]\n",
        "Token has not been saved to git credential helper.\n",
        "\u2713 Vocabulary saved: ./nepali-gector-style-token-level-tag-for-ged/gec_vocabulary.json\n",
        "\u2713 Final dataset and vocabulary saved to ./nepali-gector-style-token-level-tag-for-ged\n",
        "\n",
        "======================================================================\n",
        "STEP 7: GENERATING README.MD DATASET CARD\n",
        "======================================================================\n",
        "\u2713 README.md saved to ./nepali-gector-style-token-level-tag-for-ged/README.md\n",
        "\n",
        "======================================================================\n",
        "STEP 8: UPLOADING TO HUGGING FACE HUB\n",
        "======================================================================\n",
        "Authenticating as DipeshChaudhary...\n",
        "Cannot authenticate through git-credential as no helper is defined on your machine.\n",
        "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
        "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
        "\n",
        "git config --global credential.helper store\n",
        "\n",
        "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\n",
        "\u2713 Authentication successful.\n",
        "Creating repository: DipeshChaudhary/nepali-gector-style-token-level-tag-for-ged\n",
        "\u2713 Repository created or already exists.\n",
        "Uploading all files from ./nepali-gector-style-token-level-tag-for-ged...\n",
        "Processing\u2007Files\u2007(57\u2007/\u200757)\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007:\u2007100%\n",
        "\u200727.7GB\u2007/\u200727.7GB,\u20072.14GB/s\u2007\u2007\n",
        "New\u2007Data\u2007Upload\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007:\u2007\n",
        "\u2007\u20070.00B\u2007/\u2007\u20070.00B,\u2007\u20070.00B/s\u2007\u2007\n",
        "\u2007\u2007...ged/train/data-00044-of-00045.arrow:\u2007100%\n",
        "\u2007\u2007492MB\u2007/\u2007\u2007492MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00000-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00001-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00002-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00003-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00004-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00005-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00006-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00008-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\u2007\u2007...alidation/data-00007-of-00009.arrow:\u2007100%\n",
        "\u2007\u2007462MB\u2007/\u2007\u2007462MB\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\u2007\n",
        "\n",
        "\u2705 UPLOAD COMPLETE!\n",
        "\n",
        "======================================================================\n",
        "STEP 9: VERIFYING UPLOAD\n",
        "======================================================================\n",
        "Loading dataset from Hub: DipeshChaudhary/nepali-gector-style-token-level-tag-for-ged\n",
        "README.md:\u2007\n",
        "\u20072.48k/?\u2007[00:00<00:00,\u2007127kB/s]\n",
        "Resolving\u2007data\u2007files:\u2007100%\n",
        "\u200746/46\u2007[00:00<00:00,\u20072476.77it/s]\n",
        "Downloading\u2007data:\u2007100%\n",
        "\u200745/45\u2007[01:49<00:00,\u2007\u20071.91s/files]\n",
        "train/data-00000-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007407MB/s]\n",
        "train/data-00001-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007230MB/s]\n",
        "train/data-00002-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007312MB/s]\n",
        "train/data-00003-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007441MB/s]\n",
        "train/data-00004-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007533MB/s]\n",
        "train/data-00005-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007396MB/s]\n",
        "train/data-00006-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007264MB/s]\n",
        "train/data-00007-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007262MB/s]\n",
        "train/data-00008-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007250MB/s]\n",
        "train/data-00009-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007309MB/s]\n",
        "train/data-00010-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007287MB/s]\n",
        "train/data-00011-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007385MB/s]\n",
        "train/data-00012-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007301MB/s]\n",
        "train/data-00013-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007218MB/s]\n",
        "train/data-00014-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:04<00:00,\u2007223MB/s]\n",
        "train/data-00015-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007372MB/s]\n",
        "train/data-00016-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007321MB/s]\n",
        "train/data-00017-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007303MB/s]\n",
        "train/data-00018-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007294MB/s]\n",
        "train/data-00019-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:03<00:00,\u2007223MB/s]\n",
        "train/data-00020-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:03<00:00,\u2007218MB/s]\n",
        "train/data-00021-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007257MB/s]\n",
        "train/data-00022-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:03<00:00,\u2007247MB/s]\n",
        "train/data-00023-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007244MB/s]\n",
        "train/data-00024-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007199MB/s]\n",
        "train/data-00025-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:03<00:00,\u2007259MB/s]\n",
        "train/data-00026-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007304MB/s]\n",
        "train/data-00027-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007255MB/s]\n",
        "train/data-00028-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007243MB/s]\n",
        "train/data-00029-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007313MB/s]\n",
        "train/data-00030-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007342MB/s]\n",
        "train/data-00031-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007334MB/s]\n",
        "train/data-00032-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007320MB/s]\n",
        "train/data-00033-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007312MB/s]\n",
        "train/data-00034-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007427MB/s]\n",
        "train/data-00035-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007300MB/s]\n",
        "train/data-00036-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007313MB/s]\n",
        "train/data-00037-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007406MB/s]\n",
        "train/data-00038-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007384MB/s]\n",
        "train/data-00039-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:02<00:00,\u2007345MB/s]\n",
        "train/data-00040-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007330MB/s]\n",
        "train/data-00041-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007331MB/s]\n",
        "train/data-00042-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007328MB/s]\n",
        "train/data-00043-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007389MB/s]\n",
        "train/data-00044-of-00045.arrow:\u2007100%\n",
        "\u2007492M/492M\u2007[00:01<00:00,\u2007358MB/s]\n",
        "validation/data-00000-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:01<00:00,\u2007429MB/s]\n",
        "validation/data-00001-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:01<00:00,\u2007354MB/s]\n",
        "validation/data-00002-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:01<00:00,\u2007369MB/s]\n",
        "validation/data-00003-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:01<00:00,\u2007362MB/s]\n",
        "validation/data-00004-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:01<00:00,\u2007274MB/s]\n",
        "validation/data-00005-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:02<00:00,\u2007247MB/s]\n",
        "validation/data-00006-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:03<00:00,\u2007210MB/s]\n",
        "validation/data-00007-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:01<00:00,\u2007286MB/s]\n",
        "validation/data-00008-of-00009.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:02<00:00,\u2007255MB/s]\n",
        "test/data-00000-of-00003.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:02<00:00,\u2007301MB/s]\n",
        "test/data-00001-of-00003.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:02<00:00,\u2007313MB/s]\n",
        "test/data-00002-of-00003.arrow:\u2007100%\n",
        "\u2007462M/462M\u2007[00:01<00:00,\u2007323MB/s]\n",
        "Generating\u2007train\u2007split:\u2007\n",
        "\u200713008711/0\u2007[01:29<00:00,\u2007313522.31\u2007examples/s]\n",
        "Generating\u2007validation\u2007split:\u2007\n",
        "\u20072439231/0\u2007[00:10<00:00,\u200790949.60\u2007examples/s]\n",
        "Generating\u2007test\u2007split:\u2007\n",
        "\u2007813050/0\u2007[00:05<00:00,\u2007118811.33\u2007examples/s]\n",
        "Loading\u2007dataset\u2007shards:\u2007100%\n",
        "\u200745/45\u2007[00:00<00:00,\u200785.84it/s]\n",
        "\u2713 Verification successful! Dataset structure on Hub:\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
        "        num_rows: 13008711\n",
        "    })\n",
        "    validation: Dataset({\n",
        "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
        "        num_rows: 2439231\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['input_ids', 'attention_mask', 'labels', 'is_correct', 'tag_stats'],\n",
        "        num_rows: 813050\n",
        "    })\n",
        "})\n",
        "\n",
        "======================================================================\n",
        "ALL STEPS COMPLETE.\n",
        "======================================================================\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}