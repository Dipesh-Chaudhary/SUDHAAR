{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "af82678d",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.4.1)\n",
            "Requirement already satisfied: accelerate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.11.0)\n",
            "Requirement already satisfied: evaluate in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.4.6)\n",
            "Requirement already satisfied: scikit-learn in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.3.2)\n",
            "Requirement already satisfied: wandb in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.22.3)\n",
            "Requirement already satisfied: huggingface_hub in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
            "Requirement already satisfied: pandas in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: psutil in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from accelerate) (7.1.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from accelerate) (2.8.0+cu128)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (4.23.4)\n",
            "Requirement already satisfied: pydantic<3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from wandb) (2.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: seqeval in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from seqeval) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate evaluate scikit-learn wandb huggingface_hub\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForTokenClassification, \n",
        "    TrainingArguments, Trainer, DataCollatorForTokenClassification,\n",
        "    EarlyStoppingCallback, TrainerCallback\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import evaluate\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "from huggingface_hub import login, create_repo, HfApi\n",
        "import wandb\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"IRIIS-RESEARCH/RoBERTa_Nepali_125M\"\n",
        "# DATASET_NAME = \"DipeshChaudhary/error-type-nepali-ged-dataset\"\n",
        "DATASET_NAME1 = \"DipeshChaudhary/nepali-gector-style-token-level-tag-for-ged\"\n",
        "ERROR_TYPE_MODEL_HUB_ID = \"DipeshChaudhary/nepali-gec-error-type-classifier\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d7264d5b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c7035232d1c4a0280fb78e69f3592d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/46 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d14329cd25f48c48e2ed9ba9e414cb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/45 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset1 = load_dataset(DATASET_NAME1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# HF_TOKEN = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Login to Hugging Face\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5d16e1eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download vocabulary\n",
        "from huggingface_hub import hf_hub_download\n",
        "vocab_path = hf_hub_download(\n",
        "    repo_id=\"DipeshChaudhary/nepali-gector-style-token-level-tag-for-ged\",\n",
        "    filename=\"gec_vocabulary.json\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "01683dd4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded vocabulary: 10 tags\n"
          ]
        }
      ],
      "source": [
        "class GECVocabulary:\n",
        "    def __init__(self, vocab_path):\n",
        "        with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            self.tag_to_id = data[\"tag_to_id\"]\n",
        "            self.id_to_tag = {int(k): v for k, v in data[\"id_to_tag\"].items()}\n",
        "    \n",
        "    def get_tag_name(self, tag_id):\n",
        "        return self.id_to_tag.get(tag_id, \"$UNKNOWN\")\n",
        "    \n",
        "    def get_id(self, tag_name):\n",
        "        return self.tag_to_id.get(tag_name, 9)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tag_to_id)\n",
        "\n",
        "vocabulary = GECVocabulary(vocab_path)\n",
        "print(f\"âœ… Loaded vocabulary: {len(vocabulary)} tags\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Using 26 CPUs\n",
            "ðŸ“¦ Batch size: 1024\n",
            "ðŸ¤— Error Type Model Hub: DipeshChaudhary/nepali-gec-error-type-classifier\n"
          ]
        }
      ],
      "source": [
        "# Hardware setup\n",
        "NUM_CPUS = 26\n",
        "BATCH_SIZE = 1024 \n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "print(f\"ðŸš€ Using {NUM_CPUS} CPUs\")\n",
        "print(f\"ðŸ“¦ Batch size: {BATCH_SIZE}\")\n",
        "print(f\"ðŸ¤— Error Type Model Hub: {ERROR_TYPE_MODEL_HUB_ID}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ===========================================\n",
        "### ERROR TYPE CLASSIFIER PREPARATION\n",
        "### ============================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "64835cc2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset1['train']['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STAGE 2: ERROR TYPE CLASSIFIER - FIXED VERSION\n",
            "============================================================\n",
            "ðŸ”„ Remapping error labels to 0-6 range...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STAGE 2: ERROR TYPE CLASSIFIER - FIXED VERSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# First, let's fix the dataset by mapping labels to the correct range\n",
        "def remap_error_labels(example):\n",
        "    \"\"\"Remap original labels (1-7) to (0-6) and filter out others\"\"\"\n",
        "    remapped_labels = []\n",
        "    for label in example['labels']:\n",
        "        if label == -100:  # Keep padding\n",
        "            remapped_labels.append(-100)\n",
        "        elif 1 <= label <= 7:  # Map DELETE(1)-MERGE_PREV(7) to 0-6\n",
        "            remapped_labels.append(label - 1)\n",
        "        else:  # Filter out KEEP(0), SPLIT(8), UNKNOWN(9)\n",
        "            remapped_labels.append(-100)\n",
        "    return {'remapped_labels': remapped_labels}\n",
        "\n",
        "print(\"ðŸ”„ Remapping error labels to 0-6 range...\")\n",
        "error_dataset_remapped = dataset1.map(\n",
        "    remap_error_labels,\n",
        "    batched=False,\n",
        "    num_proc=NUM_CPUS,\n",
        "    desc=\"Remapping error labels\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "86751f21",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_dataset_remapped['train']['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a5ffdb81",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " 1,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_dataset_remapped['train']['remapped_labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93714878",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Remove the old labels column and rename\n",
        "error_dataset_final = error_dataset_remapped.remove_columns(['labels']).rename_column('remapped_labels', 'labels')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a4e51bdd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " 1,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_dataset_final['train']['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Verifying label ranges after remapping...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remapped unique labels: [0, 1, 2, 3, 4, 5, 6]\n",
            "Label range: 0 to 6\n",
            "âœ… All labels are in valid range 0-6\n"
          ]
        }
      ],
      "source": [
        "# Verification\n",
        "print(\"ðŸ” Verifying label ranges after remapping...\")\n",
        "def verify_label_ranges(dataset_split):\n",
        "    all_labels = []\n",
        "    for example in dataset_split.select(range(50000)):\n",
        "        for label in example['labels']:\n",
        "            if label != -100:\n",
        "                all_labels.append(label)\n",
        "    \n",
        "    unique_labels = sorted(set(all_labels))\n",
        "    print(f\"Remapped unique labels: {unique_labels}\")\n",
        "    print(f\"Label range: {min(unique_labels)} to {max(unique_labels)}\")\n",
        "    \n",
        "    # Should only contain 0-6\n",
        "    valid_labels = set(range(7))\n",
        "    invalid_labels = [label for label in unique_labels if label not in valid_labels and label != -100]\n",
        "    if invalid_labels:\n",
        "        print(f\"âŒ STILL INVALID LABELS: {invalid_labels}\")\n",
        "    else:\n",
        "        print(\"âœ… All labels are in valid range 0-6\")\n",
        "\n",
        "verify_label_ranges(error_dataset_final['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b0941c1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "76ab83fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute metrics for error type classification\n",
        "def compute_error_type_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    \n",
        "    # We only care about error positions (labels != 0)\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "    \n",
        "    for prediction, label in zip(predictions, labels):\n",
        "        for p, l in zip(prediction, label):\n",
        "            if l != -100 and l != 0:  # Only error positions (ignore KEEP and padding)\n",
        "                true_predictions.append(p)\n",
        "                true_labels.append(l)\n",
        "    \n",
        "    if len(true_predictions) == 0:\n",
        "        return {'accuracy': 0, 'macro_f1': 0}\n",
        "    \n",
        "    accuracy = (np.array(true_predictions) == np.array(true_labels)).mean()\n",
        "    \n",
        "    # Per-class metrics for error types\n",
        "    error_type_names = [\"DELETE\", \"REPLACE\", \"APPEND\", \"SWAP_NEXT\", \"SWAP_PREV\", \"MERGE_NEXT\", \"MERGE_PREV\"]\n",
        "    error_type_ids = [0,1, 2, 3, 4, 5, 6]\n",
        "    \n",
        "    # Filter to only include present classes\n",
        "    present_classes = [cls for cls in error_type_ids if cls in true_labels]\n",
        "    if present_classes:\n",
        "        report = classification_report(\n",
        "            true_labels, true_predictions, \n",
        "            labels=present_classes,\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "        macro_f1 = report['macro avg']['f1-score']\n",
        "    else:\n",
        "        macro_f1 = 0\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'macro_f1': macro_f1,\n",
        "        'num_error_tokens': len(true_predictions)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Calculating error type weights...\n",
            "\n",
            "Error distribution from 10,000 examples:\n",
            "  $DELETE (0): 7 (0.09%)\n",
            "  $REPLACE (1): 2,847 (34.69%)\n",
            "  $APPEND (2): 2,196 (26.76%)\n",
            "  $SWAP_NEXT (3): 1,577 (19.22%)\n",
            "  $SWAP_PREV (4): 1,577 (19.22%)\n",
            "  $MERGE_NEXT (5): 1 (0.01%)\n",
            "  $MERGE_PREV (6): 1 (0.01%)\n",
            "\n",
            "Final class weights:\n",
            "  $DELETE: 5.00\n",
            "  $REPLACE: 0.41\n",
            "  $APPEND: 0.53\n",
            "  $SWAP_NEXT: 0.74\n",
            "  $SWAP_PREV: 0.74\n",
            "  $MERGE_NEXT: 5.00\n",
            "  $MERGE_PREV: 5.00\n"
          ]
        }
      ],
      "source": [
        "def get_error_type_weights(dataset_split, vocabulary):\n",
        "    \"\"\"Get error type weights - simple and reliable approach\"\"\"\n",
        "    print(\"ðŸ“Š Calculating error type weights...\")\n",
        "    \n",
        "    # Using a reasonable sample size\n",
        "    sample_size = min(10000, len(dataset_split))\n",
        "    sample = dataset_split.select(range(sample_size))\n",
        "    \n",
        "    error_counts = Counter()\n",
        "    \n",
        "    # Count error tokens directly from labels\n",
        "    for i in range(len(sample)):\n",
        "        example = sample[i]\n",
        "        for label in example['labels']:\n",
        "            if label not in [-100] and 0 <= label <= 6:  # Error tokens we care about\n",
        "                error_counts[label] += 1\n",
        "    \n",
        "    total_errors = sum(error_counts.values())\n",
        "    \n",
        "    print(f\"\\nError distribution from {sample_size:,} examples:\")\n",
        "    for tag_id in sorted(error_counts.keys()):\n",
        "        tag_name = vocabulary.get_tag_name(tag_id+1)\n",
        "        count = error_counts[tag_id]\n",
        "        percentage = (count / total_errors) * 100 if total_errors > 0 else 0\n",
        "        print(f\"  {tag_name} ({tag_id}): {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Calculate balanced weights\n",
        "    class_weights = {}\n",
        "    for tag_id in sorted(error_counts.keys()):  # All error types we want\n",
        "        count = error_counts.get(tag_id, 1)\n",
        "        if total_errors > 0:\n",
        "            weight = total_errors / (7 * count)  # 7 error types\n",
        "            class_weights[tag_id] = min(weight, 5.0)  # Reasonable cap\n",
        "        else:\n",
        "            class_weights[tag_id] = 1.0\n",
        "    \n",
        "    print(\"\\nFinal class weights:\")\n",
        "    for tag_id in sorted(class_weights.keys()):\n",
        "        tag_name = vocabulary.get_tag_name(tag_id+1)\n",
        "        print(f\"  {tag_name}: {class_weights[tag_id]:.2f}\")\n",
        "    \n",
        "    return class_weights, error_counts\n",
        "\n",
        "# Use this - it will work\n",
        "error_weights, error_counts = get_error_type_weights(error_dataset_final['train'], vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ddaab624",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({1: 2847, 2: 2196, 3: 1577, 4: 1577, 0: 7, 5: 1, 6: 1})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Custom loss for error type classification\n",
        "class WeightedErrorLoss(nn.Module):\n",
        "    def __init__(self, class_weights):\n",
        "        super().__init__()\n",
        "        self.class_weights = torch.tensor(list(class_weights.values()), dtype=torch.float32)\n",
        "        self.ce_loss = nn.CrossEntropyLoss(weight=self.class_weights, reduction='none')\n",
        "    \n",
        "    def forward(self, logits, labels):\n",
        "        # Move weights to correct device\n",
        "        self.class_weights = self.class_weights.to(logits.device)\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = self.ce_loss(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "        \n",
        "        # Mask: only compute loss for error tokens (labels != 0 and != -100)\n",
        "        mask = (labels.view(-1) != -100) & (labels.view(-1) != 0)\n",
        "        masked_loss = loss * mask.float()\n",
        "        \n",
        "        # Average only over error tokens\n",
        "        num_error_tokens = mask.sum()\n",
        "        if num_error_tokens > 0:\n",
        "            return masked_loss.sum() / num_error_tokens\n",
        "        else:\n",
        "            return masked_loss.sum()  # Returns 0 if no errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## trying aother things - end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“¥ Loading model for error type classification...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error types: {0: '$DELETE', 1: '$REPLACE', 2: '$APPEND', 3: '$SWAP_NEXT', 4: '$SWAP_PREV', 5: '$MERGE_NEXT', 6: '$MERGE_PREV'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at IRIIS-RESEARCH/RoBERTa_Nepali_125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸ“¥ Loading model for error type classification...\")\n",
        "error_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# We'll use only the error types that exist: 1-7 (DELETE to MERGE_PREV)\n",
        "error_id2label = {id-1: name for id, name in vocabulary.id_to_tag.items() if id in [1,2,3,4,5,6,7]}\n",
        "error_label2id = {name: id for id, name in error_id2label.items()}\n",
        "\n",
        "print(f\"Error types: {error_id2label}\")\n",
        "\n",
        "error_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(error_id2label),\n",
        "    id2label=error_id2label,\n",
        "    label2id=error_label2id,\n",
        ")\n",
        "\n",
        "# Apply custom loss\n",
        "error_model.loss_fct = WeightedErrorLoss(error_weights)\n",
        "\n",
        "# Data collator\n",
        "error_data_collator = DataCollatorForTokenClassification(\n",
        "    tokenizer=error_tokenizer,\n",
        "    padding=True,\n",
        "    max_length=128,\n",
        "    pad_to_multiple_of=8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "effa0506",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '$DELETE',\n",
              " 1: '$REPLACE',\n",
              " 2: '$APPEND',\n",
              " 3: '$SWAP_NEXT',\n",
              " 4: '$SWAP_PREV',\n",
              " 5: '$MERGE_NEXT',\n",
              " 6: '$MERGE_PREV'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bef6587c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'$DELETE': 0,\n",
              " '$REPLACE': 1,\n",
              " '$APPEND': 2,\n",
              " '$SWAP_NEXT': 3,\n",
              " '$SWAP_PREV': 4,\n",
              " '$MERGE_NEXT': 5,\n",
              " '$MERGE_PREV': 6}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "error_training_args = TrainingArguments(\n",
        "    output_dir=\"./error-type-model\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # More epochs for finer classification\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-6,  # Lower learning rate for fine-grained task\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    \n",
        "    # Evaluation & Saving\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # Hub Uploading\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=ERROR_TYPE_MODEL_HUB_ID,\n",
        "    hub_strategy=\"every_save\",\n",
        "    hub_token=HF_TOKEN,\n",
        "    \n",
        "    # Optimization\n",
        "    dataloader_num_workers=NUM_CPUS,\n",
        "    dataloader_pin_memory=True,\n",
        "    fp16=True,\n",
        "    tf32=True,\n",
        "    \n",
        "    # Metrics\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_macro_f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to= None,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9305ad9c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94d7ebf93c52480080eb1f0bcc65a13d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=26):   0%|          | 0/13008711 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f62c8c97e644445081096aaa1b3491d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=26):   0%|          | 0/2439231 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f3d5fe61c624fce8c55578602aef087",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=26):   0%|          | 0/813050 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f44e4f616fce4be4980d16de18e992cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter (num_proc=26):   0%|          | 0/13008711 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f6a94a4423045f8909ffe5abf2bce71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter (num_proc=26):   0%|          | 0/2439231 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5beb1b2c4e474efa92c59bd4f7d25a6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter (num_proc=26):   0%|          | 0/813050 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filtered_dataset = error_dataset_final.map(\n",
        "    lambda batch: {\n",
        "        \"keep_mask\": [not v for v in batch[\"is_correct\"]]\n",
        "    },\n",
        "    batched=True,\n",
        "    num_proc=NUM_CPUS\n",
        ").filter(\n",
        "    lambda example: example[\"keep_mask\"],\n",
        "    num_proc=NUM_CPUS\n",
        ").remove_columns([\"keep_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-100,\n",
              " -100,\n",
              " 1,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100,\n",
              " -100]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_dataset['test']['labels'][999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "error_trainer = Trainer(\n",
        "    model=error_model,\n",
        "    args=error_training_args,\n",
        "    train_dataset=filtered_dataset[\"train\"],\n",
        "    eval_dataset=filtered_dataset[\"validation\"],\n",
        "    data_collator=error_data_collator,\n",
        "    tokenizer=error_tokenizer,\n",
        "    compute_metrics=compute_error_type_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Training error type classifier...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m9804234394d\u001b[0m (\u001b[33m9804234394d-Everest Engineering College\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20251110_173426-f4epqjzj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface/runs/f4epqjzj' target=\"_blank\">spring-jazz-35</a></strong> to <a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface' target=\"_blank\">https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface/runs/f4epqjzj' target=\"_blank\">https://wandb.ai/9804234394d-Everest%20Engineering%20College/huggingface/runs/f4epqjzj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19056' max='19056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19056/19056 2:04:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Num Error Tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.594400</td>\n",
              "      <td>0.426629</td>\n",
              "      <td>0.829424</td>\n",
              "      <td>0.539446</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.368200</td>\n",
              "      <td>0.282098</td>\n",
              "      <td>0.891865</td>\n",
              "      <td>0.588158</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.297200</td>\n",
              "      <td>0.235052</td>\n",
              "      <td>0.911127</td>\n",
              "      <td>0.602718</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.261800</td>\n",
              "      <td>0.207750</td>\n",
              "      <td>0.922088</td>\n",
              "      <td>0.610916</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.237900</td>\n",
              "      <td>0.190043</td>\n",
              "      <td>0.929187</td>\n",
              "      <td>0.616294</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.221700</td>\n",
              "      <td>0.178010</td>\n",
              "      <td>0.933868</td>\n",
              "      <td>0.619842</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>0.167541</td>\n",
              "      <td>0.937823</td>\n",
              "      <td>0.622804</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>0.160428</td>\n",
              "      <td>0.940477</td>\n",
              "      <td>0.624811</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.192000</td>\n",
              "      <td>0.154629</td>\n",
              "      <td>0.942523</td>\n",
              "      <td>0.626339</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.183600</td>\n",
              "      <td>0.150248</td>\n",
              "      <td>0.944283</td>\n",
              "      <td>0.627649</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.146632</td>\n",
              "      <td>0.945556</td>\n",
              "      <td>0.628570</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.176300</td>\n",
              "      <td>0.143062</td>\n",
              "      <td>0.946877</td>\n",
              "      <td>0.629576</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.172000</td>\n",
              "      <td>0.140794</td>\n",
              "      <td>0.947623</td>\n",
              "      <td>0.630132</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.167200</td>\n",
              "      <td>0.138268</td>\n",
              "      <td>0.948605</td>\n",
              "      <td>0.631719</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.167300</td>\n",
              "      <td>0.136843</td>\n",
              "      <td>0.949163</td>\n",
              "      <td>0.632135</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.164600</td>\n",
              "      <td>0.135507</td>\n",
              "      <td>0.949573</td>\n",
              "      <td>0.633314</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.162900</td>\n",
              "      <td>0.134901</td>\n",
              "      <td>0.949859</td>\n",
              "      <td>0.632667</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.163100</td>\n",
              "      <td>0.134184</td>\n",
              "      <td>0.950101</td>\n",
              "      <td>0.633703</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.162100</td>\n",
              "      <td>0.134003</td>\n",
              "      <td>0.950165</td>\n",
              "      <td>0.634618</td>\n",
              "      <td>1934543</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12926244737247b0b07e16bf21475470",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd7c075ba84466aa124e2089301b386",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ce8c8315cd4459480145ca64645d16e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3a06f1e3524458dad01051b30d9d596",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload: |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Error type model training completed!\n",
            "Final error type metrics: {'train_runtime': 7470.6076, 'train_samples_per_second': 2611.979, 'train_steps_per_second': 2.551, 'total_flos': 1.2747329669015378e+18, 'train_loss': 0.24652013110073548, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "print(\"ðŸš€ Training error type classifier...\")\n",
        "error_train_result = error_trainer.train()\n",
        "error_trainer.save_model()\n",
        "error_trainer.push_to_hub()\n",
        "\n",
        "print(\"âœ… Error type model training completed!\")\n",
        "print(f\"Final error type metrics: {error_train_result.metrics}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c79c5208",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (vis_env)",
      "language": "python",
      "name": "vis_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
